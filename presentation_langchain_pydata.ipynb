{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Podstawowe instalacje i ustawienie środowiska"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain openai tiktoken youtube-transcript-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "with open(\"credentials.json\") as f:\n",
    "    credentials = json.load(f)\n",
    "\n",
    "openai_key = credentials[\"openai-key\"]\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_key"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Załadowanie i przygotowanie danych"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document Loaders obsługuje wiele różnych źródeł danych: z tych oczywistych mogą być to dokumenty wordowskie, csv, pandasowe ramki, pdfy. Z tych mniej oczywistych będą takie jak notatki z EverNote, Notion, konwersacje z ChataGPT, AZLyrics, dane z Confluence. Jeżeli chodzi o możliwość bezpośredniego połączenia się z danymi, które trzymamy w chmurze, document loaders umożliwa połoczęnie z Azurowym Blob Storagem czy S3 z AWS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import YoutubeLoader\n",
    "from langchain.schema import Document\n",
    "\n",
    "yt_link = [\n",
    "    \"https://www.youtube.com/watch?v=X4-hu3vZAOg&list=PLGVZCDnMOq0qT0MXnci7VBSF-U-0WaQ-w&index=1\",\n",
    "    \"https://www.youtube.com/watch?v=DvDxS4uKj5Q&list=PLGVZCDnMOq0qT0MXnci7VBSF-U-0WaQ-w&index=2\",\n",
    "    \"https://www.youtube.com/watch?v=wiGkV37Kbxk\",\n",
    "    \"https://www.youtube.com/watch?v=ux9OLDBR9RE&list=PLGVZCDnMOq0qgYUt0yn7F80wmzCnj2dEq&index=5\",\n",
    "    \"https://www.youtube.com/watch?v=AMJEnkA0YrE&list=PLGVZCDnMOq0qgYUt0yn7F80wmzCnj2dEq&index=12\",\n",
    "    \"https://www.youtube.com/watch?v=AGg9NH2XpYs&list=PLGVZCDnMOq0qgYUt0yn7F80wmzCnj2dEq&index=20\",\n",
    "    \"https://www.youtube.com/watch?v=BeBVdjENBZo\",\n",
    "]\n",
    "\n",
    "documents = []\n",
    "for link in yt_link:\n",
    "    loader_yt = YoutubeLoader.from_youtube_url(link)\n",
    "    data_yt = loader_yt.load()\n",
    "    for item in data_yt:\n",
    "        documents.append(\n",
    "            Document(page_content=item.page_content, metadata={\"url\": link})\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"thank you very much for being my guinea pigs for uh for today it's always fun to be the first uh first of a pi data london session so as mentioned my name is anders i am norwegian i live in copenhagen and have lived lots of different places i have a background in japanese business actually so i've had a lot of hats i've currently i am as mentioned the head of the python enablement team at modis management so my job is to make sure that we have all the python tools we need build up python infrastructure do courses trainings workshops sort of be internal consultants for all the teams at uds management so you can imagine a lot of quants a lot of trading i'm sure you all you londoners know all about that um so i'm this is the get repo that we will be working with today so i'll give you everyone a second to clone it if you want to follow along of course that is of course optional i'm a bit curious before we start so i assume everyone here is working with sql or else i assume you wouldn't be here but what kind of database are we working with can i see a hand for postgres okay i'm a sequel oracle poor souls uh did i forget anything what's in the other category here all right excellent so we got like the three main ones uh we got covered here so the great thing is that sql alchemy works with all of these and it lets you sort of write independently of your database to some extent so if we jump over to the intro over here so sql to me is actually a very very old project in python terms i tried to do some digging but the history of it has sort of been moved from garrett to to github and back and forth but it's been around for a very long time it's started as a way of layering on top of these other python database tools that we had but before we get into that the reason that we use sql academy is compared to many other tools that other languages use or also just in python sql alchemy doesn't try to hide away the sql so it is a sql toolkit it's not a sql abstraction language it's not a dsl it does support the object relational mapper which lets you do some nice things in hiding ways of the complexity but again it doesn't try to hide away the sequel and you can always dive down into the sql parts because one of the big complaints about orms is that they are a leaky abstraction that they they mismatch sort of what you're trying to do in your code and what's actually happening in the database and that leads to a lot of problems down the line so to sort of support this this setup sql alchemy has two main parts i briefly mentioned them but we have the core and we have the orm and we're going to dive into what those two things are but first a bit of history so pep 249 was introduced in 2001 it was actually two peps at the same time one was older than the other but they're both registered at the same date and that basically specified how do we talk to database libraries from python it's standardized standardized python api to do that and that let all the data providers write a database connector that could be used to do the same thing so if we're taking an example that i actually built into python so sqlite as you are probably using everyday because it's actually the biggest most popular database in the world it's a single file embedded database so you can you know import sqlite in python it's right there which is great for a lot of use cases so we pass it a file name and then we can use the db api so usually the pattern looks like this you have a with something.connect to the connection string this makes sure that at the end of this block we close the connection and clean up the any lingering connections to the database and then we can execute some string which is then sent to the database parse the sql and then you get the result back yes of course one more so if you run this nothing really happened right it feels like it but if i look in my file system over here and hit the refresh button i now have this local.db again this is a sqlite specific thing because it's a file database it creates that database so i executed this create table and now i can reconnect to it and then i can you know do an insert and then i can do select and if i look at this result i got this list of tuples back one in text which is the one i inserted so you see already here a few things that the ddb api is doing for us it has this insertion syntax to avoid sql injection uh it also parametrizes the types in the sense that you get something back something binary back from the database the db api spec takes that and turns it into python type so if i do type of this you can see i'm getting a tuple so it's a list of tuples with python types so already there you know there's a bunch of magic happening right but this this probably doesn't look great to work with um so why is that well the curious we're dealing with raw strings as programmers as pythonistas manipulating strings is not why we're into writing python right also if i was to take this toy example and switch over to my sql server database i would have to install a completely different library rewrite all the text and actually change the code because i have to use the sqlites connection so if it was cx oracle for [Music] oracle databases or pi odbc for mysql i would actually have to change my code uh and i'm actually not using any of the power of python again it's all string manipulation right so that doesn't feel very iconic so let's try to do the same thing but this time with sql alchemy uh i personally like to import sql alchemy as essay uh sort of like the pi data vibe of two letter uh abbreviations uh mainly because yeah you do use the sequel alchemist course stuff a lot that is a personal preference and it's of course up to you so the first thing we need to talk about is the various objects that that sql output provides so the first one we need to know about is the metadata so the metadata keeps track of all the different tables that a sql company is in charge of so it's sort of a global register for sql alchemy to work with and where does all this look up so when you're doing joins and stuff like that it also is quite handy in the sense that it knows the order in which things need to be created so if you have dependencies between your tables the metadata registry knows how to figure that out if you need to create a table or drop a table so of course now we need a table that is of course the fundamental unit of working with sql so we have a table object here notice i gave it the name and then i also passed it the meta the meta object here and that's to register it into the metadata and then i have this column construct here which basically is how uh sql algorithm keeps track of the types of the various columns so i declare a column name column one of type integer and i call them a call name call two or type string of course this is the very simple basic types but just matching the previous example so now we have a table the third part that we need is the engine so this is sort of the the active parts this is the part that actually talks to the database and translates sql alchemy things into sql in the database so the fundamental construct is the create engine that gives you an engine object from a connection string so one caveat here we are in the transition period in c gloccamy again it's a very long proj a very old project 2.0 is right around the corner so to smooth the transition what they've done is let you set this future equals true flag that lets you opt into cql commute 2.0 behavior so if you pass this feature equals true you're basically working in 2.0 when that rollover happens then you'll be prepared you're not going to have any surprises it's also much nicer to work in trudeau it's much more consistent it's it's much clearer what's going on and doesn't have as many warts as the old one which is of course what happens when you go to a major version bump so we create our engine and now i'm actually ready to do something so let's write some sql so now i'm writing the same sql query as before but now i'm using python objects instead so i i do a select on the test table and that creates again a python object and the overwrite the wrapper so i get this selectable object but if i print it it actually generates the sql shows me what sql is going to be one so it gives you a lot of introspection in terms of what's going to happen sequel alchemy is not going to try to surprise you by doing something else that what it's promised to do it also overrides a bunch of operators so for example i can do this column notice i have the dot c to access the columns so it's actually called columns but because we use a lot you can also use dot c i column one equals test so that gives me this binary expression object so that note means that sql alchemy can actually translate that into some sql so this would be the equivalent sql of what i just wrote so adding a where clause is actually fairly simple i can tag on influent programming style dot where and then just write what looks like regular python this equals that and that will generate a select from test where as you would expect to run it we do a connection just as like like we did in the db api style and then we execute the sql and then we say all means get all the results so it looks the same right visually but it's actually a bit of an upgraded version because this is no longer tuple it's a sql sql alchemy engine row so this row object it has for example dot dot access so i can do dot column one in previous one it was tuples i had to do indexing which isn't very pleasant it means i can convert it directly to a dictionary which is very useful if you're trying to pass things through for example pedantic or in your api so notice here that i use dot all so dot all is sort of a go do this everything we've been doing until now is lazy until i hit that dot all so you can say that all get all of them get one that's the first guess one results but also raise an exception if there's only one so it also gets some if you're doing you know aware to look up a customer id you expect one results if there is none there is an exception one or none same with but it's okay if there's none first the same as saying you know limit one or a top one for you sql people sql server people then you can also do things like streaming results we're not going to talk about that today but know that that is an option depending on your database driver so the different uh libraries for talking databases have different options in terms of talking to the database so for example if you're working with postgres it's very nice to have server side cursors you can send your you know 10 million row results keep it in memory in the database and then ask for chunks of it at a time sql alchemy supports that fully through streaming so behind the scenes yes yeah so that's the concept of the cursor right so you uh create a cursor you send the sql to the database that cursor then contains all the data on the database side and then you're fetching sort of whatever is in that cursor until it's exhausted no there are some things where you can actually sort of connect have a streaming live update but that's that wouldn't be a server-side cursor so and again these aren't sql alchemy features per se they're just supporting features that are in modern databases so if your modern database supports it and the python database driver supports it then clock me likely also supports it so what did actually happen behind the scenes here so it generated the correct sequel for our database so sql alchemy is database aware so for example if you are in sql serverland a limit would be called top right c clock knows that it will automatically convert a limit to a top if there are any types or casting or anything like that it knows how to convert between most things there are some things that are database very specific so cqlogme does support it but in most cases it will do a conversion automatically depending on the connection string it wraps the result of the query in this row object which gives a nice access to to each row and behind the scenes which we didn't see is that hugely has a pool of connections by defaults so that means it doesn't check out and it doesn't create a new connection every time you make a call it has a pool of warm live connections that are then rotates between so that means that you you can uh sort of have lots of lots of queries going and not open hundreds of connections because you can reuse the connections in the connection pool so these are some of the things that secrecy is doing for you behind the scenes that you don't get out of the box and others so that was sort of a very brief overview of sql academy and why we like it but and now we're going to dive into the the cloud core are there any questions yeah so uh is it more for the software yeah so i mean sleek alchemy i would say is for the pythonista in general i would say anywhere that you are today writing text sequel and show hands how many here are writing text sql yeah you should probably be using sql acme instead it gives you uh all the power of python and doesn't let you make the same mistakes that you can do in text or sql so that's sort of the target audience and then like in python in general right you can start very easy and then you can build up all the layers of complexity and we're going to talk about some of those layers from text editor today [Music] you can tell them to take this class afterwards yeah but i think we can take that at the end of this okay yeah any questions on the sql alchemy part before we move on excellent uh i i get very enthusiastic when i talk about stuff so i speak quickly and fast so if i ever do need to slow down just raise your hand and then i will try to accommodate so core sequel um so then as i mentioned it's built on layers so c clock move itself is of course built on db api and the orm which we're looking later is actually built on top of core so that's why it's always important to understand your building blocks and this is probably what i would show people who are used to writing textual sql because it maps very easily to those constructs but with extra benefits so the first thing we have to do is to have some data so i have cleaned up some data from open data dk which has some data about parking available parking in various areas so the details aren't that interesting but we also have some sample data to work with so as before the first thing we need to do is actually create some tables so this is more of a closer to real life example that you might do at work or for fun if you're like me so of course we import sqlcommy we create our metadata database and then we have to create our tables just like before so now we can get a bit more interesting with the with the types because cqlogma has a rich database type system and then knows how to convert those rich types back into python types later so we have some you know ids some foreign keys some varchars integers so notice that we have we can also set up these four key relationships we can tell sql communities are primary keys we can add uniqueness identifiers all those fun things that you're used to from from sql but i'm driving it all from from python so if i run this nothing is actually going to happen everything we've done here is sort of in memory on python side we haven't talked to any databases at all cql acme does support what's called reflection so if you have an existing database and you're too lazy to type out this you can do something that's called the reflection where you pass it the engine and then it'll look up all these types and sort of give you this table back in general i recommend writing it out because you don't have to write every single column this is just the columns that you need python to be aware of again because it's all happening on this side not on the database side but it gives you an expensive schema and it also allows you to do things like migrations and easier introspection of what is it actually you have available right so in general this is what you're going to be doing so we have our facts which is the parking table with accounts per hour per year month per area we have a dim area which says something about the area where is it in the city street name that sort of thing and then we have some different parking types kind of like in enum so given that we are starting a new database because we are going to be running our own for those who have docker installed you can uncomment this and then run it i'm going to be demoing this with postgres because it's more of a gives you a bit more of a proper database feel if you do not have docker installed you want to follow along you can uncomment this bottom one then you'll be running with the sqlites which is that file database that we saw previously and that's all built into python but as mentioned i'll be running in actually let me open my database editor first who you know that pycharm has a full jet sql integration yeah it's awesome uh one of the best reasons to get a pro license actually pay for myself uh so yeah so we set up our connection string the connection string will vary depending on what database you have uh again for your oracle people i'm very sorry we use oracle it works as well so sql may use that connection stream to infer what database apis and each drivers need to load and all these things so of course you need to still have your database driver application installed in a sense that you need for postgres something like cycle pg2 for oracle cx oracle uh and pi obviously you still need to have those installed but the sql me will know which one to use based on the prefix in this case postgresql so if we look at our meta you can actually see it's a this type called facade dictionary so i can actually look up a given table in here if i could type oh it helps if i pass the right area so i can actually grab the tables that i defined out of the metadata and that's actually what's happening in behind the scenes in sequel alchemy and it's the same thing if you do the reflection it'll populate the metadata tables and you can grab them out of there but actually interact with a database again we need to create a connection make sure we set future equals true and now we can use the metadata object to create all and if i open up my database editor over here probably should have done that before you came up let's see i believe it was just postgres postgres so that it connects and we should see that we now have in the public schema we have three tables dimension partner types and fact parking so it created all those for me with the appropriate you can see the keys here we have some foreign keys maps like all that sql actually just took care of it for me which is very nice of course it has a dropball counterpart which will drop all the tables that it knows about so again dropbox is not going to drop all the tables and database only the ones defined in the metadata so if we want to insert something we can create a sql statement so i'm sure you've all written an insert into the statement before the sql acme version maps pretty closely to most of the sql constructs so essay.insert into the table the imparking types dot values the device i want to insert and that converts nicely into what you would expect and again i cannot do sql injection sql acumen does not let me do sql injection so that's a nice added benefit if you're trying to sell to your colleagues it'll automatically create these uh parameters and do parameters queries you can also do the all the tables have some helper methods to generate this sequel for you so for example the inserts we can just do it straight on the department types that insert and that'll generate the exact same sql so again to actually execute we need to open a connection in a width block general rule always always always use a width block unless you have very good reason not to i might break that rule in this demo because it's a it looks a bit prettier if i'm not opening with having with blocks all the time but always always always use with block that'll make sure you don't have lingering connections and your dbas will be very very happy with you so now i just executed that sql so if i go back to the database we would expect something right wrong sql alchemy doesn't presume that you want to actually commit the data that's because you alchemy works is something called unit of work which we'll cover in a bit but if i actually want to say yep i'm sure now i'm ready then you have to call commit everything is inside of a transaction so if i go back now and refresh now i have the data in there we can delete some rows again i have this helper method on the table dot delete and i can chain on a where clause and that will give me a classic delete from again parametrized again no sql injection i execute that make sure it commits and then we can go back and see it's gone as you would expect so this is what we call commit as you go you have to tell sql alchemy that now is the time to commit so if you do you can also call rollback so you can se you can control the date of the uh the flow of data so if you're whatever reason you're not happy then you can roll back at any points and that'll basically undo everything up until the last commit so if i do the inserts and then commit that and then delete it or roll back would i have data in my database or would i have no data in my database i would have data should we check and i do have data so of course uh i inserted i committed so that part is sort of sent to the database and persisted new transaction i run some more sql i'm not happy with that i roll back everything gets thrown out up until the last commit that gives you a lot of safety uh when you're working with with sql accommodate like you're not accidentally going to delete something unless you sort of explicitly say it tell it to commit i mean this the selection or the oh this one yeah yes exactly yeah because we you don't commit so it doesn't actually happen so the rollback is more of a i've done all these things make sure you throw them away it's an explicit way of saying don't keep it anymore because remember this data is still in it's been sent to the database it just hasn't been committed so i might have some more things that i want to do in this block afterwards that i do want to commit so it gives you that it can give you a fairly fine level of control yep over there can you just give us an example uh so actually we'll try catch is interesting we'll get back to that in a bit but it would be that you for example try optimistically to do something and you have something else to do afterwards it's not something that you really normally do because we have some other methods which are sort of safer by defaults but if you're doing something more intricate where you have multiple things talking to the database at the same time within the same transaction like you have multiple functions that are sending data to the database you generally want to have one connection being passed to all these functions and inside one function maybe you figure out that oh no actually something happened another function that means i shouldn't do this after all then you can roll back that bit and everything else sort of carry on as if it never happened so yeah so the rollback part in general you're not going to use it 80 of the time but it is being it is happening behind the scenes and a lot of things were going on to look at right now so it's very useful to know about so that comes back to this idea of unit of work so a lot of other frameworks uh especially uh in ruby like active record they work on sort of i want to modify this row and then i want to modify this row and then i want to modify this row so you clock me says well instead of doing that i'm going to get all the things that you want to do and then i'm going to then when you're ready then i'll send it all to the database and because then i can sort of optimize things bit behind the scenes uh in terms of how the queries get sent and what data gets sent and how we flush so for with sqladme instead you want to run all your sqls and then commit when you're done so for example if you're writing a web api you probably want to open your connection when you get a request do all your things then then commit and then it all gets sent to the database in sort of one unit that's why it's called one unit of work so everything starts in interaction whenever we open a new connection block then it actually starts a new transaction in the database you can also manage that connection a transaction starter manually if you want to with the begin so we can again execute this delete we can commit it we can start a new transaction execute and then roll back the transaction so do we have data do we not have data so we have data hands up do we not have data hands up excellence as you're wires to the ways of sql so yeah we don't have data because again we explicitly design manage our transaction and roll back that transaction usually we don't need this level of control it's very rare that we actually explicitly work with transactions so sql document provides this other form if you want to explicitly make sure that everything is running inside a transaction because you have multiple transactions you want to do or you just want to not have to remember to commit which is also something that can be quite nice then you can do this dot begin so dot begin will automatically create a transaction for you run the sql and if you do have an exception then it'll roll back for you if it if there is no exception it'll commit so if i run my insert my deletes i should have data no data there you go no data so committed both at the end if i have an exception here i insert raise the exception something bad happens data no data no data so that gives you a nice safety net if you have if you use this dot begin pattern it'll automatically roll back if you have a transaction so of course you can always implement your own try catch uh exception uh if you want and then you can like get more fine grains so sql locking does provide you these level of extraction abstraction depending on where you want to be uh so what if we have lots of rows that we want to insert right now we've been playing with these toy examples um including the repo there is this the csv of the actual data that we're talking about because again we just created some tables we haven't actually put any data into these databases so we just have traditional insert statement query stuff uh grab each of these csvs in a tuple together with the the table we want to insert into we open a dot begin because again this this is an example of where we have multiple things happening right we want to roll everything back if only if one of them fails so then we iterate over the tuple get the data file and the table open it grab the data as a list of dictionaries create the sql in this case just insert and then we can use this other syntax that we haven't seen before so we can do an execute pass sql and then pass the parameters explicitly so when you pass a list of dictionaries with keys matching the table columns uh sql alchemy knows okay now now i need to do an execute menu so in the db api spec there's a difference between an execute which does sort of one thing and execute many which is optimized for multiple typical inserts or deletes so sql alchemy will take care of you as long as you pass it to list of dictionaries and then you get a bunch of speed ups again depending on your db driver so if we look into our database now we should have a bunch of data and the facts so i'm not getting a count up here that's okay yeah there's some couple thousand rows in there i just have something to play around with so this is how you this is very typical insertion logic in python so replace csv file here with parquet or some other sql or a data frame or something like that so now we're ready to actually write some some logic that's the fun part right unless you're a dba or a data engineer you might not be doing a lot of insertion you might be doing more selects so i've lined up a couple of tasks that i want to get done so you're going to learn a bit about the danish geography so that's fun and then pronunciation which is even more fun i'm not danish i just happen to speak it so task one get only accounts for gilai uh lovely image pronunciation so what we can do is we can do a select and we refer to the columns with the table dot c dot table name so we want the count we want the hour and then we want all everything from the area table so that's sort of like a select star if we don't pass anything we do join to the area and we pass aware so you can see it has this like fluent chaining syntax which you might be familiar with from example working in pandas which makes it really nice to modify the the sql so if we print it you can see it looks pretty much as you would expect select a number of columns from some tables joined with another table did you notice that sql alchemy actually figured out the join key for you that is actually one of the really nice things so because we specified a foreign key relationship you do not have to worry about it you just join it and see what can you figure it out it'll tell you if you can't figure it out if for example you have multiple foreign keys or if there are some other reasons why i can't figure it out most of the cases it'll just figure it out uh so now we can actually execute it we can get all of them and we get back this row objects that's a list of a list of these rows so i can do traditional row sizing i'll get the top five and then print the dictionaries and you can see okay that looks reasonable it's probably not how you normally want to work with data if you're at pi data so of course you can throw it straight into a data frame that will get you a nice data frame because of course data frame supports lists of dictionaries and the results object can be coerced into a dictionary so the row objects can easily be corresponds to dictionary and of course pandas knows all about c me it actually uses it under the hood every time you do anything dot dot two sequel or read sequel so you can pass the sequel and the connection straight to read sql and pandas will know what to do with it uh one caveat when we're working with 2.0 pandas it doesn't quite compatible with 2.0 yet they need to do like a big rewrite of the sql internals in general so you do need to open a connection like this and then pass it to the to its equal function it doesn't know how to do that itself quite yet so let's get a bit more complicated let's actually have some mathematics so i want to know the sum of counts like how many parking spots were available per hour per city so normally you would write in sql you write some something and then do a group by uh it works pretty much the same way so sql alchemy has this magic object called a func and this func any attribute call to it will map to the corresponding sql function so sqlockem doesn't actually know about all the functions that are defined in sql server or whatever database you're working in it knows some of them so it does do some conversion but basically just takes whatever you put after that func and says okay that's the function i'll just pass it with those parameters so in this case i say that func.sum of this count column has to give it a label otherwise it's going to be anonymous and then i have to include my grouping columns i have my join and then i do a group i and again i just pass in the columns that i want to group by so i want you to take away that writing these sql core statements is very very similar to writing regular sql you should be able to fairly easily take medium complex complexity sequels and map them one to one in c clock and core to get started so if we run that just to verify that looks right and then we execute it get the data frame and we can see we have city hour pairs and a total count so this sequel i mean it's not bad but it looks like sequel uh and again we're not writing sql we're writing python so you're asking how we can convince our fellow colleagues that sql alchemy is a good idea you can actually refactor your sql how many people can say that about their text strings right so i can split out the different parts and give them variable names so i can for example pre-join the tables called table i can create this metric total count which is these sata thunk.sum and then finally i can i have some dry right i'm repeating the group by columns in both the select and the group i that's a common pane in all sql right so i can actually create a list of those columns and now i can turn my sql statement into total counts dimension calls select from table group by dimension goals so now my sql statement is quite terse but at the same time fairly readable because especially if you're working with aggregations in sql they get very long until you figure out what's actually going on at the end when you read the label so if we run this we get the same just for comparison this is what it looked like before uh let's actually print this as well that'll help so that you can tell on that line creates the same sql so as an aside this fluent style of chaining makes it really really easy to modify the sql in place so what you can do is that you can on the fly update the sql because every statement just creating a new statements so i can add a new group by column i can add a new select column so there was the group by down here three and up here and i'm not changing the original sql that's the important part so part of the power of this is that you can write reusable functions in logic that takes in a sql statement modifies it returns it and then you all of a sudden you can have you know the holy grail metrics functions group by functions that's long sql query that you have to calculate some random business objects put that into a function you don't have to look at it anymore you can just say here's the sequel add this crazy long function to it so that was on the side but it is a part of the power that we get out of using c gloccum so again we can exhibit execute it notice i'm dynamically adding a limit statement here so again i'm just modifying the sql to add a limit which gets translated into the backend sql and now i have the top 10. so now let's get even more complicated uh let's calculate some occupancy rates so the occupancy rates in this case we're defining it as available parking spots and use parking spots at a given time uh so just to check how many do you know what a cte is common table expression what about nested sub queries okay so basically i'm going to be using a cte a ct is basically a way of creating an intermediate table so you can have a select statement and you can kind of name it and then you can treat that select statement as if it was a table so that's basically what it is it looks pretty confusing the first time you use it but they're super super handy when you're writing sql in general but so in order to do the calculation right i need to match up because my fact table if we looked at over here so i have the account type and the count type is legal or counted so i actually have in my facts i have two tables sort of appended on top of each other i have all the table all the parking spots there are followed by all the parking spots are counted so i need to sort of break these two apart and put them next to each other so that i can do that that division across the tube and panda's right would be fairly simple because you can just do the two selects and then divide across them and sql it gets a bit more funky but with the power supply alchemy we can hopefully simplify this expression so that we can break it down into small parts and then just run a small sql statement at the end so one thing we can do is we have the space select statements which is basically going to be our fact table together with our parking types that'll give us the counts and whether or not it is occupied or legal i'm going to be reusing those so i can write it once and i can just refer to it so available spaces would be a a cte where we select partner types equals legal so now i have the available spaces table the counter spaces is the same except now i'm doing it with counted equal to counted my metric of interest is i have to cast two floats because we're getting a percentage uh multiply by 100 available spaces uh sorry uh counter space is divided by available spaces and label so again like that's already getting pretty long because we have to handle all this uh conversion stuff right so it's pretty nice to get that out of the way and give it a nice label so i can just say refer to it as occupancy rate and the joint condition we can no longer automatically infer it because we're creating these temporary tables there are no foreign keys for it to match right so our join statement actually gets a bit long but again we can abstract that into a variable because we have to do it by area id and join on month and join on the hour because those are the things we want to calculate by and then i can create the join so i can do available spaces join the counter spaces so now i'm mapping together and the join condition is already defined and now i can actually write my sql so now i can actually read it and say okay i want the occupancy rates the city the year month and the hour okay that makes sense in my head that's what i'm you know trying to get if i print out the sql that's a lot longer i don't know about you but i prefer the my version that's i am biased so if you run this and notice i can even pass in the inline stuff like order by so again the sql alchemy gives us a bunch of helper methods so i want order by descending the order the occupancy rates is x sorry a column so that has a desk method on it which will convert the order by into a order by descending and then i get the occupancy rate city year month an hour so if you are in scanable then sucks to be you if you're in savvy then you have lots of parking available that makes a lot more sense if your danish so i hope that you got the notion that c clock macor is sql just in python so uh how we on time thank you very much so i have a quick exercise here for you uh i'm going to give you five minutes to to give it a go so basically i want you to find all the rows where the occupancy rate is above 100. make sure you've executed all the cells before you attempt this exercise um i mean that would be nice uh if you want to start a new open source project then that sounds like a great idea no uh so so there is like the get out of jail free card which is the text construct so there is a thing called essay.text you pass it sql and then it'll treat it as if it was already compiled by sqlcommy so it would be the same as if you were to define the whole sequel in c alchemy style and then sort of have run it but it's not going to decompose it into the various objects i haven't tried if you were able to do for example the replace selectings that i actually do not know that but that's you can that you can actually try for yourself uh let's see if you can do the uh what did i do add a new group by column and uh or add a new select column so sql alchemy is quite deep when you get into it so there are lots of handy little tricks like that so if you do try it let me know i'll be curious um so give it two more minutes all right how are we looking do we need a bit more time so how many how many of you this was the first time you ever wrote an ec black me all right cool congratulations so i think we'll we'll call it there how many managed to solve the uh exercise uh i don't have the counters more if it worked you should look at the data and see that you don't have any more than uh or they're all above 100. but uh the idea is that you can chain off of the previous sql that we have that's the uh you always have to have a trick right with these questions so if i have the sql that's just a table uh dodge occupancy rates actually let's uh i was just talking about how much nicer it is to have name variables now i have to remember how to do this too right see that oh cupancy yeah so there is some uh some new things going on that are deprecated that's why you always have to be a bit careful uh but that's why we have that future thing because uh sometimes things will work and then tomorrow they're not gonna work because then c blockme2.0 comes out so here they're saying that they want me to do a ct that's fine did i misspell something yeah i did that's the dots accuracy rate there always good thing to double check so now i can do t dot c dot occupancy great that where t dot c dot so there are lots of ways you could have done this there are many ways to roam in in sql and therefore many ways your moments you glock me yeah naming things is hard guys with engine.connect r is equal to cons.execute t sql dot all data type mismatch great it helps if i actually have a work where clause and then if i throw this into dataframe that looks great to me so i hope at least you've got a good effort but as many of these questions is a trick right knowing that you can chain these methods onto the previous sequel and sql acme does help does have pretty good error messages you can see that when i did the old-fashioned way it actually warned me that hey you should be doing this with the cte when i misspelled the column name it actually told me that like this this column doesn't actually exist so cool are you guys ready for rms or do we have any questions on core mind blown sure thing feel free to give me better names so the question was will speak optimize optimizer query no it'll not change your sql query like the text of it what it will do is it'll try to optimize the way it sends it to the database um but sqladmin does not presume to know your intent when it comes to the to the sql when we talk about core uh when we talk about orm it's a bit more fuzzy but yeah that's also part of the sneaky abstraction thing we were talking about that if we try to hide like the uh sql code from the python code then it starts doing things that you don't expect and that that's part of the design principles of sql alchemy is that it doesn't try to hide anything it's not trying to be a a sql writer it's trying to be a sql yeah parsing engine let's call that cool anyone else then let's dive into the wonderful world of orms so the orm is actually where a lot of people get their starts in sql alchemy they're usually coming from something like django they're used to working in the orm there maybe now they're trying out some api language like flask or fast api and they're like where's my orm and then they find out that oh sql camera has an rm great in my opinion that's the wrong way to learn sqlcommy because again the orm does magic that's kind of what orms do they are mapping a python class to a sql row that's that's what an object relational mapper does that means that by its very nature there is magic going on to sort of convert between those two things which sounds the same thing but usually aren't so learning core first lets you always have that escape hatch of okay now i'm doing something i can just drop down into core and write it so it does it the way that i needed to do it and is one of the actually the core part in the pun features of sql alchemy that you can always switch in between because orm is just building on top of core so when would you use it uh so my rule of thumb is orms are great if you're talking about single rows so that's why it's very common in something like django or in an api you're doing request response get me this customer get me this purchase get me this uh blog post then it works great if you're only working with one road a time and manipulating that or a handful let's say then orms are great if you're working with analytics in any way you want to do counts sums aggregations of any kind core will probably be better for your use case because there's also one caveat if you were to select 100 000 rows and then convert them into python objects that's not fast converting them into pi into c clock rows is a lot faster so just keep that in mind that you want to think about tens and not thousands when you're talking about orm so again the first thing we always need to do is to define our tables that's how sql alchemy knows what's what it's got to work with so the way we do that is actually fairly different because now we're talking about classes so the classic way and i'll show you this because every single sql option tutorial you see out there is going to be using this way until everyone migrates to new 2.0 style is to run this function which returns a class and then you will subclass this derived class and then write your your own class on top of that so remember this library was written way before we had type checkers before we had linters this does not make my pie happy it does not make a lot of linting libraries happy in fact some of them have coded in special exceptions for c clock me again because c block me so pervasive but in general we don't want to return a class or even not even an instance of a class a type of a class from a function right that looks odd but that's how it was done so that's you can still do that that's not going away so you you create your your base class here and then you subclass it you always have to give it a table name and that's going to be what the name of the table in the database is going to be so the rule of thumb here is in its in your database you probably want plurals you want customers uh in orem you usually you have one customer at a time so there you're one singular one customer right and the other the rule is that a orm class has to define a primary key you're not allowed to have a rm object does not have a primary key because that's how sql alchemy keeps track of the individual objects in its own internal representation so if you can't fulfill those two things then oms are not for you but yeah but that's that's basically how you do it and when you do that behind the scenes there is again this metadata object being created it's inside of this base class and this table gets registered but in 2.0 one of the nice new features is this that they revamped this registry system so now we can create a registry instead and then this registry can be used to decorate your class with dot maps and that sort of fits in more with how we think about these things these days right we have data classes we have adders uh function classes that use this pattern uh having a class decorator is not a crazy thing to do right also fits more in with a registry pattern in general but the same rules still apply you still need a table name you still need your primary key and in this case we just have a random address but it's just a class so i can add a nice wrapper to it that shows me sort of the key data and then you know that will be printed out the other nice thing that it does i remember remember i told you that om is just building on top of core it just auto creates the c clock record table in this under table attribute so it's right there you can you can go grab it and work with it and do whatever you need to do uh notice that we're not finding in inits so this is actually something that's because again this predates data classes and adders and all these things this is one of the first libraries that i had this i'll create and in it for you based on class variables so cyclone will automatically create an init that accepts these but you are free to define your own in it if you want to either restrict what you want to accept or if you want to do some you know post-processing of the input arguments before you create your for instance that's perfectly fine let's finish up our data models so we have we have our address we have let's have a purchase i have this table arcs here orms don't really work well in notebooks they're not meant for that so par oh sorry sql alchemy will complain that i'm going to be redefining these classes throughout the notebook uh because it messes with this internal registry so i have this extend existing just means that it's okay i know what i'm doing i'm opting out of this checking behavior it'll still complain with a warning so don't worry about that but just to let you know so just ignore this line this is for notebook purposes only but yeah so the definition is pretty much exactly the same i have a foreign key i use these columns i have for example a decimal the nice thing is i can add these high pins again we're writing modern python i can add tie pins to my attributes here clock me doesn't care about them yet there's actually a feature that's coming up that you can do type hinting to instead of writing these essay.com things but it's not a feature yet so right now it's just for my own my own sake it helps when we write our code we can also use more exotic types like we can use an enum so for the customer these customers have a status they can be gold silver bronze so then we can set the status to be of type enum and then we get to the cool part about orms which is relationships which is to me the killer feature so notice here that i define two relationships address so a customer has an address and purchases a customer has many purchases so what does that actually mean so i still have my database up and running if you don't then you have to recreate it so we have a new keyword here and again oms are magical they will create and run sql on your behalf that you have no control over per se so when you're getting used to working with the orm you can turn on this echo equals true then sql alchemy will print out every sql statement it is writing and running so that gives you that extra level of control when you're developing that's okay it is actually doing what i think it's doing because especially with orms they're going to be doing a lot more calls than you think they're doing and then i'll do the same thing notice that on the map registry that's where my metadata lives now so i can just access it through there and here's our first example of all the stuff that clapping actually does for me some highlights is that actually checks do these tables actually exist already if not skip them in a cross database agnostic way and then actually runs all the creates and creates all the primary keys and foreign keys and all those references as we saw before and now i'm ready to actually create some data so you just create an instance it's just instances of class i have john jane here they're not i haven't again i haven't talked to the database yet they're just a class with attributes dot name and dot status everything you would expect from a python class nothing up my sleeves so the way you talk to your database in rm is with a thing called session you're no longer working with a direct connection object you're working with a session because the section adds some extra abstractions on top like keeping track of what is already loaded into memory what is not loading primary keys into memory so it can access various data so we need the session thing instead but the pattern looks very similar in the sense that we create an instance of session give it the engine in a width block and that's we're just going to call session.execute instead of con.execute so pretty close but not quite so in the session we add these objects and then c-clock we make sure that uh now it knows to keep track of them again i still have to actively commit it's still an opt-in commit so if i commit that you can see we get the inserts and notice speaking of optimizing sql queries i have from this i get one insert that is parametrized i'm not getting two insert statements and that's that unit of work in in action because the session knows oh i have two of these two of these things that aren't committed then i can optimize what i'm sending to the database and write the sql in a way that optimizes for that inserts so instead of sending 100 it will send one insert with many parameters so now we can add an address and we can do that in many ways but one way is to just assign it to the attributes so now john has an address so now if i re-add john and commit that so you clock me will notice hey john now has an address he didn't have one before and doesn't exist so let's insert into addresses this new address and then we select again to refresh the session because we update we have to make sure that john himself hasn't changed and then we update him to insert the address id of the newly created address so if we jump into the database and look at what actually happened so this is the magic part of the ograms right having to keep track of relationships across tables is a pain so i have my address and then i have john with address id of one that's pretty cool notice that jane here she doesn't have an address yet so she's just null so john goes shopping and he buys a magic potion so when i create purchase because i set this relationship up uh if i go to purchase notice i didn't set one on purchase but yeah the only thing about these sql queries is they get pretty long you know when you're doing the echo we were down here somewhere but this still works so why do you think that works that's a bit odd well if we look at our customer here you notice i set a relationship to the address table backref equals customer so backref is back reference that means that what also does is it registers on the address class that hey you have a relationship to customer you know about until now but now you do which means that it adds that attributes to the address class or to the purchaser class and any customer so you can do it manually you can explicitly set a relationship on each side and then do back populates which will say like there is a matching relationship here or you can set the back graph here and then it will just add that attribute for you so that's more up to your encoding style explicit versus implicit and all those fun things but now if i add the potion we see the same thing we select the customer so we got a fresh copy of the customer we insert into the purchases so this is going on and then we insert it with the user id but again that's the caveat right it's doing some magic for me but i wouldn't be able to guess how many sql statements that simple operation would have done if i wasn't quite very familiar with c locally so in this case i did two it could have done lots more if there were if there was more data so caveat emptor always make sure that you know what your orem is doing we can also just create a purchase so we have our magic hat here then i can again connect john to the session because if john's not in the session then sql acme doesn't know how to look them up to make sure we get the right data and it'll just complain and tell you hey i'm not in a session so please add me so when i have john then i can append him to my purchases attributes and then because equal me just rep represents these relationships as a list i can read and then i can commit and again we're generating a bunch of sql statements three in this case but it also does some caching which is nice to know that's a new feature in 2.0 it does more aggressive caching of your queries so the compilation of the sql statements gets cached by sql alchemy and that's what it's telling you here that it it's already seen in the sql so it doesn't actually need to uh to recompile it so now if i go back into my purchases i have two purchases and they are both are mapped to the correct user id so this is this was my mind blown moment when i started working with clockme because doing this by hand sucks that is like the worst part of writing sql for some kind of api is keeping track of all these related objects and making sure that they're in sync at all times so if it takes a few more sql calls to to make that happen by magic then i'm perfectly comfortable with that so we have some data and how do you actually now get that data back so the big thing that happened to 2.0 was a realignment if you're looking at pre 2.0 then you would have different syntaxes for selecting depending on whether it was orm or whether it was core which meant that every time you googled how do i do this sequel in sql alchemy you always have to make sure that you're either looking at the orm version or the core version which was really annoying in 2.0 it's been aligned now we're using the same essay that selects whether or not it's in orm or whether or not it's in core and that's fantastic only one api to learn right so in the end you just say essay to select customer it's the same thing as if it was a table in core uh filter by is just a nice syntax instead of saying dot where name equals equals jane you can do filter by keyword arguments value and just as the same so it's a convenience method and it creates the the correct sql so and again the execution is almost the same we're just using a session instead of the connection but we do session execute one or none and we get our jane back so especially one or none is something you're going to be using a lot more if you're using the orm because then you're typically in a context where either the customer exists or he or he does not so you're fine either way but you just need to know but if there are two customers with the same value then you probably did something wrong so that's a nice level of safety you get there so and we have our customer chain but wait that looks a bit odd what happened here so again things changed a bit in 2.0 and if this was 1.4 it would not look like this you'd get your chain back and everyone wouldn't be happy but now we're aligning our syntax across core and orm and in core when you do select you get back a row object and you have to access each value in it exact same thing for orm so if i do jane 0 then i get jane back out i actually don't remember is a name customer yeah so that basically get back your result a row that has one one column in it and that column is called customer and it has j inside so obviously that's a bit clunky so we introduced this thing called scalars so scalar was actually a thing back in the day as well but it wasn't necessary for rm now it is so scalar just says i want to whatever is in the first row in the first column that's the value i want and if you're using multiple then it's just whatever's in the first column that's the value that i want so you tack on scalars to your executes do one or none and now we get back what we would expect so because this is such a common pattern in 2.0 we have a bunch of these helpers which do it in a single method call instead so you see scalar one or none scalar all scalar versions of all the various operations but just know that just it just means adding the scalars modifier to the previous results access so one more way because obviously in python we like to have multiple ways if you know the customer or the primary key then you can use a sort of shortcut syntax which lets the session optimize the query uh because it checks do i have this already uh can i reuse something or do i have to go fetch it so if you do session.get on the table and pass the primary key then i'll do this optimization to to get out the customer and we get the same customer back so now that we have our customers we can start asking questions about the related attributes we set up these relationships and we probably did it because we care about them so let's put john back into the session and then let's ask about his purchases and his address so we got a bunch of sql being generated but all i did was just access an attribute so what's up with that and if i do this with a regular attribute there's no sql being emitted at all so this comes back into what relationships uh actually are and why you have to be a bit careful you see we have here one selecting the customers we have selected purchases and then we select the addresses so these two dots these three actually created three sql statements even though you're just doing attribute lookup so this idea of relationship loading is pretty important if you want to really take advantage of relationships so by default sql alchemy orms are lazy if you're fetching a customer you're only getting the rows from the customer table because joins are can be expensive so sql acme doesn't want to presume that you automatically want to get the join with all the addresses or pushes imagine like if you're a twitter and you get one you know get all your posts or it's a massive join right so by default it's lazy because that's the safest if you don't know what's going on though it becomes quite dangerous because then you're doing attribute looks up every attribute lookups everywhere not knowing that you're constantly sending sql statements to the database which will be fine when you're in dev and then you hit production all of a sudden your scale increases by 10 folds 100 folds and your sql query is increased by a thousand volts so to get around this you can explicitly in a query you can explicitly say i want this to be done in a different way so when i select customer here i i can pass this options and i can say that the address relationship should be done with a joined load and the purchases relationship should then with it select in load where customers name equals to john so if we look at this now so the if i get the customer here let's see if you can find that then you see i do a left outer join with addresses so that's this is this join load so then in one sql statement it's doing a left join with the relationship table and getting the all those in because obviously we want to have all the customers but we also don't have an address that's perfectly legitimate query the other the other option is to do a select in so there we can select the purchases where purchase a user id in primary key so there you're reducing it to two selected statements one to get the customer and then another one to select all the purchases where the purchase id is in this part this is a custom id so again it comes down to two flavors and uh it's more up to you uh sort of exactly how it should be done because you're the one who knows the database again sequel oximeter's philosophy is we don't leak we don't hide away the sql you are still in charge of the database you know best about how the database is structured we're not going to make all these assumptions for you we can also set these up from the start so when i define my relationship i can also say how should this by default be done so i can say lazy equals joined we'll do the join load or laser equals selection select in which you'll lose the selected load so you can predefine that if you know that every time you want that and then you can opt into other behaviors depending on your query so i'm going to skip over this exercise so you're free to do this for yourself i just want to talk about what some of the nice things that orams give you on top of relationships which mainly are that orms are just classes and classes are first class contracts in python it gives you a lot of nice features for example you can create a mix in so let's say you want to have two columns on every table that says when was this created and when was the last updated normally you'd have to do that by hand but you can just specify mixin so if you haven't seen mixing before it's just a name for a pattern where you're not supposed to override it it doesn't have behavior it just adds some extra stuff to your class and then if i list my columns here you can see i now have two more created ads and some over here should be the yeah there last updated and created that gets automatically added to this class [Music] so the other nice thing is we can define a class method so this is actually a pretty powerful construct in python how you define alternative constructors especially if you're you know doing serialization you get some json you want to convert it into your class the easiest way to do that is to create a class method that defines okay from dict i'll take addict and then creates the class based on that so you can control that behavior so this is the warning i was talking about that i'm redefining the class which you should not be doing for demo purposes only so now i can create a user object from dictionary and sort of control that behavior we also have properties like is admin i can of course manually write the code to check if this user is an admin or not or i can add a property which makes it look like it's an attribute so now i can do is admin false the nice thing is that sql alchemy actually lets you hook into this behavior in in python so we have this thing called a hybrid property so hybrid property just means that we tell c blocking hey this thing you can actually use it in the sql statement as well so instead of defining a property we define a hybrid property and then we create all that and we get a user we add that user and now i can do in my sql statements notice i'm using the class because it's a class property and not an instance property is admin and that will compile into or uses that role equals role so now i have a nice class property that i can use both after i get my instance and in my sql statements we can also verify those mixed in while we're at it so last updated 9 23 26 and 9 23 50. so those are actually working as intended as well sometimes the logic doesn't matter as easily in this case property was a simple equals that gets over overloaded in sql alchemy and gives you as a where clause often is not that neat so let's say for example that we have that the role can be one of many so we're doing it in that doesn't translate to sequel to me so we need to specify the sql that gets run in the sql world to match the result from the python world the way you do that is you specify your hyper property this is the python code and then you can do another one it has to be named the same and then you you say the proper name dot expression that tells people alchemy this is where the sql expression you should use comes from and it's on the class level so you get a class and i can say class that role in this is the way you say usually create an in statements in a c locate this list so now i have my user i have a property that i can access on the instance or i can use the is validated so just notice that because we're using a bit of cpu magic it can't quite show you what's going to happen because it needs to do some extra processing of the query parameters that come in so which is why you get this funky post-compiled world but it does work and we get back jade who was validated so that's how you can do that so we only have five minutes left uh so i just want quickly to mention hybrid methods basically the same as a hyper property it just works for methods instead so let's say we have our purchase here we want to define the ability to calculate an roi on the on our user here sorry so basically take the sum of all those purchases and divide it by the total cost and that'll give us an roi on this customer so i need to have some information in order to calculate this which is why i can't be property uh i need the cost so i execute oh boom boom i'm sorry there's some duplication here don't write your demos late at night people uh here we go so here we have a jnr user with the purchase of zero i can also have a plain jane regular method here uh purchase which just adds to the purchase cost notice that you can do nice things like this where you can abstract away some logic you can pass into the active session and then actually work with the instance directly on the on the method so this case i update the purchases with the item cost and then i persist that into the database and then this roi method down here [Music] so now we have some purchases so if we looked at here notice it's actually selecting from the database the purchases and then i can actually calculate the roi this is on the instance so it's not doing any sql but i can then use it in a where statements so where roi is greater than one for example and that translates it into the correct statements so i can execute that and then i get back my chain and then her roi just to show that it was working is greater than one and with that i will close this session so thank you very much for your time so we have uh three minutes before alisa i get kicked out you're free to say you can ask some questions on here or otherwise just meet me outside i'll be here all weekend so good timing so yeah we got a question over there with things like data glasses and things like that but this one's keep some extra functionality like a good wrapper method that is there to combine to choose somehow uh yeah actually uh that's one of the imperatives for changing the way of registering uh classes was explicitly to support data class and adders so you can actually have your data class and then tell sql alchemy this is the class that i want you to use as sort of the serialization class and then it just works peanut butter and chocolate so i would refer to the docs considering we have two minutes um but if you do see glacomy so the only it sounds amazing but it is a bit more verbose because obviously it has to do a bit more work oh that's the one i wanted see and i will plug the c clock documentation they are fantastic um they do a lot but a lot of work on it so this whole section here is how you can do exactly that so basically you have to redefine everything twice because you need the c blocking representation of the table and your data class representation so it's not as pretty but you don't have to do it in line like you could have your tables definition somewhere else and then you know just say set the table equal to it because this is the piece of magic that the mapper also does is to generate this for you it's um oh we have one all the way back over there excellent question so you're asking about the data immigration database migration tool for student uh so the same guy who wrote c block me also wrote alembic uh so basically alembic lets you is able to read all your sql community definitions and then automatically create a migration based on that by introspecting the target database your models and creating a diff of the two and then writing the ddl to create the difference between the two then also register when you apply it it also registers in the database hey version blah blah blah is the current one so it can always keep them in sync knowing what version your database is in so then you can do like git style upgrade downgrade between versions of your database so so check that out that's uh the current gold standard for or at least for c locally thank you very much [Applause]\", metadata={'url': 'https://www.youtube.com/watch?v=X4-hu3vZAOg&list=PLGVZCDnMOq0qT0MXnci7VBSF-U-0WaQ-w&index=1'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def get_video_title(youtube_url):\n",
    "    response = requests.get(youtube_url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    title_tag = soup.find(\"title\")\n",
    "    video_title = title_tag.text.strip()\n",
    "    return video_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_with_title = []\n",
    "for item in documents:\n",
    "    title = get_video_title(item.metadata[\"url\"])\n",
    "    url = item.metadata[\"url\"]\n",
    "    docs_with_title.append(\n",
    "        Document(page_content=item.page_content, metadata={\"url\": url, \"title\": title})\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"thank you very much for being my guinea pigs for uh for today it's always fun to be the first uh first of a pi data london session so as mentioned my name is anders i am norwegian i live in copenhagen and have lived lots of different places i have a background in japanese business actually so i've had a lot of hats i've currently i am as mentioned the head of the python enablement team at modis management so my job is to make sure that we have all the python tools we need build up python infrastructure do courses trainings workshops sort of be internal consultants for all the teams at uds management so you can imagine a lot of quants a lot of trading i'm sure you all you londoners know all about that um so i'm this is the get repo that we will be working with today so i'll give you everyone a second to clone it if you want to follow along of course that is of course optional i'm a bit curious before we start so i assume everyone here is working with sql or else i assume you wouldn't be here but what kind of database are we working with can i see a hand for postgres okay i'm a sequel oracle poor souls uh did i forget anything what's in the other category here all right excellent so we got like the three main ones uh we got covered here so the great thing is that sql alchemy works with all of these and it lets you sort of write independently of your database to some extent so if we jump over to the intro over here so sql to me is actually a very very old project in python terms i tried to do some digging but the history of it has sort of been moved from garrett to to github and back and forth but it's been around for a very long time it's started as a way of layering on top of these other python database tools that we had but before we get into that the reason that we use sql academy is compared to many other tools that other languages use or also just in python sql alchemy doesn't try to hide away the sql so it is a sql toolkit it's not a sql abstraction language it's not a dsl it does support the object relational mapper which lets you do some nice things in hiding ways of the complexity but again it doesn't try to hide away the sequel and you can always dive down into the sql parts because one of the big complaints about orms is that they are a leaky abstraction that they they mismatch sort of what you're trying to do in your code and what's actually happening in the database and that leads to a lot of problems down the line so to sort of support this this setup sql alchemy has two main parts i briefly mentioned them but we have the core and we have the orm and we're going to dive into what those two things are but first a bit of history so pep 249 was introduced in 2001 it was actually two peps at the same time one was older than the other but they're both registered at the same date and that basically specified how do we talk to database libraries from python it's standardized standardized python api to do that and that let all the data providers write a database connector that could be used to do the same thing so if we're taking an example that i actually built into python so sqlite as you are probably using everyday because it's actually the biggest most popular database in the world it's a single file embedded database so you can you know import sqlite in python it's right there which is great for a lot of use cases so we pass it a file name and then we can use the db api so usually the pattern looks like this you have a with something.connect to the connection string this makes sure that at the end of this block we close the connection and clean up the any lingering connections to the database and then we can execute some string which is then sent to the database parse the sql and then you get the result back yes of course one more so if you run this nothing really happened right it feels like it but if i look in my file system over here and hit the refresh button i now have this local.db again this is a sqlite specific thing because it's a file database it creates that database so i executed this create table and now i can reconnect to it and then i can you know do an insert and then i can do select and if i look at this result i got this list of tuples back one in text which is the one i inserted so you see already here a few things that the ddb api is doing for us it has this insertion syntax to avoid sql injection uh it also parametrizes the types in the sense that you get something back something binary back from the database the db api spec takes that and turns it into python type so if i do type of this you can see i'm getting a tuple so it's a list of tuples with python types so already there you know there's a bunch of magic happening right but this this probably doesn't look great to work with um so why is that well the curious we're dealing with raw strings as programmers as pythonistas manipulating strings is not why we're into writing python right also if i was to take this toy example and switch over to my sql server database i would have to install a completely different library rewrite all the text and actually change the code because i have to use the sqlites connection so if it was cx oracle for [Music] oracle databases or pi odbc for mysql i would actually have to change my code uh and i'm actually not using any of the power of python again it's all string manipulation right so that doesn't feel very iconic so let's try to do the same thing but this time with sql alchemy uh i personally like to import sql alchemy as essay uh sort of like the pi data vibe of two letter uh abbreviations uh mainly because yeah you do use the sequel alchemist course stuff a lot that is a personal preference and it's of course up to you so the first thing we need to talk about is the various objects that that sql output provides so the first one we need to know about is the metadata so the metadata keeps track of all the different tables that a sql company is in charge of so it's sort of a global register for sql alchemy to work with and where does all this look up so when you're doing joins and stuff like that it also is quite handy in the sense that it knows the order in which things need to be created so if you have dependencies between your tables the metadata registry knows how to figure that out if you need to create a table or drop a table so of course now we need a table that is of course the fundamental unit of working with sql so we have a table object here notice i gave it the name and then i also passed it the meta the meta object here and that's to register it into the metadata and then i have this column construct here which basically is how uh sql algorithm keeps track of the types of the various columns so i declare a column name column one of type integer and i call them a call name call two or type string of course this is the very simple basic types but just matching the previous example so now we have a table the third part that we need is the engine so this is sort of the the active parts this is the part that actually talks to the database and translates sql alchemy things into sql in the database so the fundamental construct is the create engine that gives you an engine object from a connection string so one caveat here we are in the transition period in c gloccamy again it's a very long proj a very old project 2.0 is right around the corner so to smooth the transition what they've done is let you set this future equals true flag that lets you opt into cql commute 2.0 behavior so if you pass this feature equals true you're basically working in 2.0 when that rollover happens then you'll be prepared you're not going to have any surprises it's also much nicer to work in trudeau it's much more consistent it's it's much clearer what's going on and doesn't have as many warts as the old one which is of course what happens when you go to a major version bump so we create our engine and now i'm actually ready to do something so let's write some sql so now i'm writing the same sql query as before but now i'm using python objects instead so i i do a select on the test table and that creates again a python object and the overwrite the wrapper so i get this selectable object but if i print it it actually generates the sql shows me what sql is going to be one so it gives you a lot of introspection in terms of what's going to happen sequel alchemy is not going to try to surprise you by doing something else that what it's promised to do it also overrides a bunch of operators so for example i can do this column notice i have the dot c to access the columns so it's actually called columns but because we use a lot you can also use dot c i column one equals test so that gives me this binary expression object so that note means that sql alchemy can actually translate that into some sql so this would be the equivalent sql of what i just wrote so adding a where clause is actually fairly simple i can tag on influent programming style dot where and then just write what looks like regular python this equals that and that will generate a select from test where as you would expect to run it we do a connection just as like like we did in the db api style and then we execute the sql and then we say all means get all the results so it looks the same right visually but it's actually a bit of an upgraded version because this is no longer tuple it's a sql sql alchemy engine row so this row object it has for example dot dot access so i can do dot column one in previous one it was tuples i had to do indexing which isn't very pleasant it means i can convert it directly to a dictionary which is very useful if you're trying to pass things through for example pedantic or in your api so notice here that i use dot all so dot all is sort of a go do this everything we've been doing until now is lazy until i hit that dot all so you can say that all get all of them get one that's the first guess one results but also raise an exception if there's only one so it also gets some if you're doing you know aware to look up a customer id you expect one results if there is none there is an exception one or none same with but it's okay if there's none first the same as saying you know limit one or a top one for you sql people sql server people then you can also do things like streaming results we're not going to talk about that today but know that that is an option depending on your database driver so the different uh libraries for talking databases have different options in terms of talking to the database so for example if you're working with postgres it's very nice to have server side cursors you can send your you know 10 million row results keep it in memory in the database and then ask for chunks of it at a time sql alchemy supports that fully through streaming so behind the scenes yes yeah so that's the concept of the cursor right so you uh create a cursor you send the sql to the database that cursor then contains all the data on the database side and then you're fetching sort of whatever is in that cursor until it's exhausted no there are some things where you can actually sort of connect have a streaming live update but that's that wouldn't be a server-side cursor so and again these aren't sql alchemy features per se they're just supporting features that are in modern databases so if your modern database supports it and the python database driver supports it then clock me likely also supports it so what did actually happen behind the scenes here so it generated the correct sequel for our database so sql alchemy is database aware so for example if you are in sql serverland a limit would be called top right c clock knows that it will automatically convert a limit to a top if there are any types or casting or anything like that it knows how to convert between most things there are some things that are database very specific so cqlogme does support it but in most cases it will do a conversion automatically depending on the connection string it wraps the result of the query in this row object which gives a nice access to to each row and behind the scenes which we didn't see is that hugely has a pool of connections by defaults so that means it doesn't check out and it doesn't create a new connection every time you make a call it has a pool of warm live connections that are then rotates between so that means that you you can uh sort of have lots of lots of queries going and not open hundreds of connections because you can reuse the connections in the connection pool so these are some of the things that secrecy is doing for you behind the scenes that you don't get out of the box and others so that was sort of a very brief overview of sql academy and why we like it but and now we're going to dive into the the cloud core are there any questions yeah so uh is it more for the software yeah so i mean sleek alchemy i would say is for the pythonista in general i would say anywhere that you are today writing text sequel and show hands how many here are writing text sql yeah you should probably be using sql acme instead it gives you uh all the power of python and doesn't let you make the same mistakes that you can do in text or sql so that's sort of the target audience and then like in python in general right you can start very easy and then you can build up all the layers of complexity and we're going to talk about some of those layers from text editor today [Music] you can tell them to take this class afterwards yeah but i think we can take that at the end of this okay yeah any questions on the sql alchemy part before we move on excellent uh i i get very enthusiastic when i talk about stuff so i speak quickly and fast so if i ever do need to slow down just raise your hand and then i will try to accommodate so core sequel um so then as i mentioned it's built on layers so c clock move itself is of course built on db api and the orm which we're looking later is actually built on top of core so that's why it's always important to understand your building blocks and this is probably what i would show people who are used to writing textual sql because it maps very easily to those constructs but with extra benefits so the first thing we have to do is to have some data so i have cleaned up some data from open data dk which has some data about parking available parking in various areas so the details aren't that interesting but we also have some sample data to work with so as before the first thing we need to do is actually create some tables so this is more of a closer to real life example that you might do at work or for fun if you're like me so of course we import sqlcommy we create our metadata database and then we have to create our tables just like before so now we can get a bit more interesting with the with the types because cqlogma has a rich database type system and then knows how to convert those rich types back into python types later so we have some you know ids some foreign keys some varchars integers so notice that we have we can also set up these four key relationships we can tell sql communities are primary keys we can add uniqueness identifiers all those fun things that you're used to from from sql but i'm driving it all from from python so if i run this nothing is actually going to happen everything we've done here is sort of in memory on python side we haven't talked to any databases at all cql acme does support what's called reflection so if you have an existing database and you're too lazy to type out this you can do something that's called the reflection where you pass it the engine and then it'll look up all these types and sort of give you this table back in general i recommend writing it out because you don't have to write every single column this is just the columns that you need python to be aware of again because it's all happening on this side not on the database side but it gives you an expensive schema and it also allows you to do things like migrations and easier introspection of what is it actually you have available right so in general this is what you're going to be doing so we have our facts which is the parking table with accounts per hour per year month per area we have a dim area which says something about the area where is it in the city street name that sort of thing and then we have some different parking types kind of like in enum so given that we are starting a new database because we are going to be running our own for those who have docker installed you can uncomment this and then run it i'm going to be demoing this with postgres because it's more of a gives you a bit more of a proper database feel if you do not have docker installed you want to follow along you can uncomment this bottom one then you'll be running with the sqlites which is that file database that we saw previously and that's all built into python but as mentioned i'll be running in actually let me open my database editor first who you know that pycharm has a full jet sql integration yeah it's awesome uh one of the best reasons to get a pro license actually pay for myself uh so yeah so we set up our connection string the connection string will vary depending on what database you have uh again for your oracle people i'm very sorry we use oracle it works as well so sql may use that connection stream to infer what database apis and each drivers need to load and all these things so of course you need to still have your database driver application installed in a sense that you need for postgres something like cycle pg2 for oracle cx oracle uh and pi obviously you still need to have those installed but the sql me will know which one to use based on the prefix in this case postgresql so if we look at our meta you can actually see it's a this type called facade dictionary so i can actually look up a given table in here if i could type oh it helps if i pass the right area so i can actually grab the tables that i defined out of the metadata and that's actually what's happening in behind the scenes in sequel alchemy and it's the same thing if you do the reflection it'll populate the metadata tables and you can grab them out of there but actually interact with a database again we need to create a connection make sure we set future equals true and now we can use the metadata object to create all and if i open up my database editor over here probably should have done that before you came up let's see i believe it was just postgres postgres so that it connects and we should see that we now have in the public schema we have three tables dimension partner types and fact parking so it created all those for me with the appropriate you can see the keys here we have some foreign keys maps like all that sql actually just took care of it for me which is very nice of course it has a dropball counterpart which will drop all the tables that it knows about so again dropbox is not going to drop all the tables and database only the ones defined in the metadata so if we want to insert something we can create a sql statement so i'm sure you've all written an insert into the statement before the sql acme version maps pretty closely to most of the sql constructs so essay.insert into the table the imparking types dot values the device i want to insert and that converts nicely into what you would expect and again i cannot do sql injection sql acumen does not let me do sql injection so that's a nice added benefit if you're trying to sell to your colleagues it'll automatically create these uh parameters and do parameters queries you can also do the all the tables have some helper methods to generate this sequel for you so for example the inserts we can just do it straight on the department types that insert and that'll generate the exact same sql so again to actually execute we need to open a connection in a width block general rule always always always use a width block unless you have very good reason not to i might break that rule in this demo because it's a it looks a bit prettier if i'm not opening with having with blocks all the time but always always always use with block that'll make sure you don't have lingering connections and your dbas will be very very happy with you so now i just executed that sql so if i go back to the database we would expect something right wrong sql alchemy doesn't presume that you want to actually commit the data that's because you alchemy works is something called unit of work which we'll cover in a bit but if i actually want to say yep i'm sure now i'm ready then you have to call commit everything is inside of a transaction so if i go back now and refresh now i have the data in there we can delete some rows again i have this helper method on the table dot delete and i can chain on a where clause and that will give me a classic delete from again parametrized again no sql injection i execute that make sure it commits and then we can go back and see it's gone as you would expect so this is what we call commit as you go you have to tell sql alchemy that now is the time to commit so if you do you can also call rollback so you can se you can control the date of the uh the flow of data so if you're whatever reason you're not happy then you can roll back at any points and that'll basically undo everything up until the last commit so if i do the inserts and then commit that and then delete it or roll back would i have data in my database or would i have no data in my database i would have data should we check and i do have data so of course uh i inserted i committed so that part is sort of sent to the database and persisted new transaction i run some more sql i'm not happy with that i roll back everything gets thrown out up until the last commit that gives you a lot of safety uh when you're working with with sql accommodate like you're not accidentally going to delete something unless you sort of explicitly say it tell it to commit i mean this the selection or the oh this one yeah yes exactly yeah because we you don't commit so it doesn't actually happen so the rollback is more of a i've done all these things make sure you throw them away it's an explicit way of saying don't keep it anymore because remember this data is still in it's been sent to the database it just hasn't been committed so i might have some more things that i want to do in this block afterwards that i do want to commit so it gives you that it can give you a fairly fine level of control yep over there can you just give us an example uh so actually we'll try catch is interesting we'll get back to that in a bit but it would be that you for example try optimistically to do something and you have something else to do afterwards it's not something that you really normally do because we have some other methods which are sort of safer by defaults but if you're doing something more intricate where you have multiple things talking to the database at the same time within the same transaction like you have multiple functions that are sending data to the database you generally want to have one connection being passed to all these functions and inside one function maybe you figure out that oh no actually something happened another function that means i shouldn't do this after all then you can roll back that bit and everything else sort of carry on as if it never happened so yeah so the rollback part in general you're not going to use it 80 of the time but it is being it is happening behind the scenes and a lot of things were going on to look at right now so it's very useful to know about so that comes back to this idea of unit of work so a lot of other frameworks uh especially uh in ruby like active record they work on sort of i want to modify this row and then i want to modify this row and then i want to modify this row so you clock me says well instead of doing that i'm going to get all the things that you want to do and then i'm going to then when you're ready then i'll send it all to the database and because then i can sort of optimize things bit behind the scenes uh in terms of how the queries get sent and what data gets sent and how we flush so for with sqladme instead you want to run all your sqls and then commit when you're done so for example if you're writing a web api you probably want to open your connection when you get a request do all your things then then commit and then it all gets sent to the database in sort of one unit that's why it's called one unit of work so everything starts in interaction whenever we open a new connection block then it actually starts a new transaction in the database you can also manage that connection a transaction starter manually if you want to with the begin so we can again execute this delete we can commit it we can start a new transaction execute and then roll back the transaction so do we have data do we not have data so we have data hands up do we not have data hands up excellence as you're wires to the ways of sql so yeah we don't have data because again we explicitly design manage our transaction and roll back that transaction usually we don't need this level of control it's very rare that we actually explicitly work with transactions so sql document provides this other form if you want to explicitly make sure that everything is running inside a transaction because you have multiple transactions you want to do or you just want to not have to remember to commit which is also something that can be quite nice then you can do this dot begin so dot begin will automatically create a transaction for you run the sql and if you do have an exception then it'll roll back for you if it if there is no exception it'll commit so if i run my insert my deletes i should have data no data there you go no data so committed both at the end if i have an exception here i insert raise the exception something bad happens data no data no data so that gives you a nice safety net if you have if you use this dot begin pattern it'll automatically roll back if you have a transaction so of course you can always implement your own try catch uh exception uh if you want and then you can like get more fine grains so sql locking does provide you these level of extraction abstraction depending on where you want to be uh so what if we have lots of rows that we want to insert right now we've been playing with these toy examples um including the repo there is this the csv of the actual data that we're talking about because again we just created some tables we haven't actually put any data into these databases so we just have traditional insert statement query stuff uh grab each of these csvs in a tuple together with the the table we want to insert into we open a dot begin because again this this is an example of where we have multiple things happening right we want to roll everything back if only if one of them fails so then we iterate over the tuple get the data file and the table open it grab the data as a list of dictionaries create the sql in this case just insert and then we can use this other syntax that we haven't seen before so we can do an execute pass sql and then pass the parameters explicitly so when you pass a list of dictionaries with keys matching the table columns uh sql alchemy knows okay now now i need to do an execute menu so in the db api spec there's a difference between an execute which does sort of one thing and execute many which is optimized for multiple typical inserts or deletes so sql alchemy will take care of you as long as you pass it to list of dictionaries and then you get a bunch of speed ups again depending on your db driver so if we look into our database now we should have a bunch of data and the facts so i'm not getting a count up here that's okay yeah there's some couple thousand rows in there i just have something to play around with so this is how you this is very typical insertion logic in python so replace csv file here with parquet or some other sql or a data frame or something like that so now we're ready to actually write some some logic that's the fun part right unless you're a dba or a data engineer you might not be doing a lot of insertion you might be doing more selects so i've lined up a couple of tasks that i want to get done so you're going to learn a bit about the danish geography so that's fun and then pronunciation which is even more fun i'm not danish i just happen to speak it so task one get only accounts for gilai uh lovely image pronunciation so what we can do is we can do a select and we refer to the columns with the table dot c dot table name so we want the count we want the hour and then we want all everything from the area table so that's sort of like a select star if we don't pass anything we do join to the area and we pass aware so you can see it has this like fluent chaining syntax which you might be familiar with from example working in pandas which makes it really nice to modify the the sql so if we print it you can see it looks pretty much as you would expect select a number of columns from some tables joined with another table did you notice that sql alchemy actually figured out the join key for you that is actually one of the really nice things so because we specified a foreign key relationship you do not have to worry about it you just join it and see what can you figure it out it'll tell you if you can't figure it out if for example you have multiple foreign keys or if there are some other reasons why i can't figure it out most of the cases it'll just figure it out uh so now we can actually execute it we can get all of them and we get back this row objects that's a list of a list of these rows so i can do traditional row sizing i'll get the top five and then print the dictionaries and you can see okay that looks reasonable it's probably not how you normally want to work with data if you're at pi data so of course you can throw it straight into a data frame that will get you a nice data frame because of course data frame supports lists of dictionaries and the results object can be coerced into a dictionary so the row objects can easily be corresponds to dictionary and of course pandas knows all about c me it actually uses it under the hood every time you do anything dot dot two sequel or read sequel so you can pass the sequel and the connection straight to read sql and pandas will know what to do with it uh one caveat when we're working with 2.0 pandas it doesn't quite compatible with 2.0 yet they need to do like a big rewrite of the sql internals in general so you do need to open a connection like this and then pass it to the to its equal function it doesn't know how to do that itself quite yet so let's get a bit more complicated let's actually have some mathematics so i want to know the sum of counts like how many parking spots were available per hour per city so normally you would write in sql you write some something and then do a group by uh it works pretty much the same way so sql alchemy has this magic object called a func and this func any attribute call to it will map to the corresponding sql function so sqlockem doesn't actually know about all the functions that are defined in sql server or whatever database you're working in it knows some of them so it does do some conversion but basically just takes whatever you put after that func and says okay that's the function i'll just pass it with those parameters so in this case i say that func.sum of this count column has to give it a label otherwise it's going to be anonymous and then i have to include my grouping columns i have my join and then i do a group i and again i just pass in the columns that i want to group by so i want you to take away that writing these sql core statements is very very similar to writing regular sql you should be able to fairly easily take medium complex complexity sequels and map them one to one in c clock and core to get started so if we run that just to verify that looks right and then we execute it get the data frame and we can see we have city hour pairs and a total count so this sequel i mean it's not bad but it looks like sequel uh and again we're not writing sql we're writing python so you're asking how we can convince our fellow colleagues that sql alchemy is a good idea you can actually refactor your sql how many people can say that about their text strings right so i can split out the different parts and give them variable names so i can for example pre-join the tables called table i can create this metric total count which is these sata thunk.sum and then finally i can i have some dry right i'm repeating the group by columns in both the select and the group i that's a common pane in all sql right so i can actually create a list of those columns and now i can turn my sql statement into total counts dimension calls select from table group by dimension goals so now my sql statement is quite terse but at the same time fairly readable because especially if you're working with aggregations in sql they get very long until you figure out what's actually going on at the end when you read the label so if we run this we get the same just for comparison this is what it looked like before uh let's actually print this as well that'll help so that you can tell on that line creates the same sql so as an aside this fluent style of chaining makes it really really easy to modify the sql in place so what you can do is that you can on the fly update the sql because every statement just creating a new statements so i can add a new group by column i can add a new select column so there was the group by down here three and up here and i'm not changing the original sql that's the important part so part of the power of this is that you can write reusable functions in logic that takes in a sql statement modifies it returns it and then you all of a sudden you can have you know the holy grail metrics functions group by functions that's long sql query that you have to calculate some random business objects put that into a function you don't have to look at it anymore you can just say here's the sequel add this crazy long function to it so that was on the side but it is a part of the power that we get out of using c gloccum so again we can exhibit execute it notice i'm dynamically adding a limit statement here so again i'm just modifying the sql to add a limit which gets translated into the backend sql and now i have the top 10. so now let's get even more complicated uh let's calculate some occupancy rates so the occupancy rates in this case we're defining it as available parking spots and use parking spots at a given time uh so just to check how many do you know what a cte is common table expression what about nested sub queries okay so basically i'm going to be using a cte a ct is basically a way of creating an intermediate table so you can have a select statement and you can kind of name it and then you can treat that select statement as if it was a table so that's basically what it is it looks pretty confusing the first time you use it but they're super super handy when you're writing sql in general but so in order to do the calculation right i need to match up because my fact table if we looked at over here so i have the account type and the count type is legal or counted so i actually have in my facts i have two tables sort of appended on top of each other i have all the table all the parking spots there are followed by all the parking spots are counted so i need to sort of break these two apart and put them next to each other so that i can do that that division across the tube and panda's right would be fairly simple because you can just do the two selects and then divide across them and sql it gets a bit more funky but with the power supply alchemy we can hopefully simplify this expression so that we can break it down into small parts and then just run a small sql statement at the end so one thing we can do is we have the space select statements which is basically going to be our fact table together with our parking types that'll give us the counts and whether or not it is occupied or legal i'm going to be reusing those so i can write it once and i can just refer to it so available spaces would be a a cte where we select partner types equals legal so now i have the available spaces table the counter spaces is the same except now i'm doing it with counted equal to counted my metric of interest is i have to cast two floats because we're getting a percentage uh multiply by 100 available spaces uh sorry uh counter space is divided by available spaces and label so again like that's already getting pretty long because we have to handle all this uh conversion stuff right so it's pretty nice to get that out of the way and give it a nice label so i can just say refer to it as occupancy rate and the joint condition we can no longer automatically infer it because we're creating these temporary tables there are no foreign keys for it to match right so our join statement actually gets a bit long but again we can abstract that into a variable because we have to do it by area id and join on month and join on the hour because those are the things we want to calculate by and then i can create the join so i can do available spaces join the counter spaces so now i'm mapping together and the join condition is already defined and now i can actually write my sql so now i can actually read it and say okay i want the occupancy rates the city the year month and the hour okay that makes sense in my head that's what i'm you know trying to get if i print out the sql that's a lot longer i don't know about you but i prefer the my version that's i am biased so if you run this and notice i can even pass in the inline stuff like order by so again the sql alchemy gives us a bunch of helper methods so i want order by descending the order the occupancy rates is x sorry a column so that has a desk method on it which will convert the order by into a order by descending and then i get the occupancy rate city year month an hour so if you are in scanable then sucks to be you if you're in savvy then you have lots of parking available that makes a lot more sense if your danish so i hope that you got the notion that c clock macor is sql just in python so uh how we on time thank you very much so i have a quick exercise here for you uh i'm going to give you five minutes to to give it a go so basically i want you to find all the rows where the occupancy rate is above 100. make sure you've executed all the cells before you attempt this exercise um i mean that would be nice uh if you want to start a new open source project then that sounds like a great idea no uh so so there is like the get out of jail free card which is the text construct so there is a thing called essay.text you pass it sql and then it'll treat it as if it was already compiled by sqlcommy so it would be the same as if you were to define the whole sequel in c alchemy style and then sort of have run it but it's not going to decompose it into the various objects i haven't tried if you were able to do for example the replace selectings that i actually do not know that but that's you can that you can actually try for yourself uh let's see if you can do the uh what did i do add a new group by column and uh or add a new select column so sql alchemy is quite deep when you get into it so there are lots of handy little tricks like that so if you do try it let me know i'll be curious um so give it two more minutes all right how are we looking do we need a bit more time so how many how many of you this was the first time you ever wrote an ec black me all right cool congratulations so i think we'll we'll call it there how many managed to solve the uh exercise uh i don't have the counters more if it worked you should look at the data and see that you don't have any more than uh or they're all above 100. but uh the idea is that you can chain off of the previous sql that we have that's the uh you always have to have a trick right with these questions so if i have the sql that's just a table uh dodge occupancy rates actually let's uh i was just talking about how much nicer it is to have name variables now i have to remember how to do this too right see that oh cupancy yeah so there is some uh some new things going on that are deprecated that's why you always have to be a bit careful uh but that's why we have that future thing because uh sometimes things will work and then tomorrow they're not gonna work because then c blockme2.0 comes out so here they're saying that they want me to do a ct that's fine did i misspell something yeah i did that's the dots accuracy rate there always good thing to double check so now i can do t dot c dot occupancy great that where t dot c dot so there are lots of ways you could have done this there are many ways to roam in in sql and therefore many ways your moments you glock me yeah naming things is hard guys with engine.connect r is equal to cons.execute t sql dot all data type mismatch great it helps if i actually have a work where clause and then if i throw this into dataframe that looks great to me so i hope at least you've got a good effort but as many of these questions is a trick right knowing that you can chain these methods onto the previous sequel and sql acme does help does have pretty good error messages you can see that when i did the old-fashioned way it actually warned me that hey you should be doing this with the cte when i misspelled the column name it actually told me that like this this column doesn't actually exist so cool are you guys ready for rms or do we have any questions on core mind blown sure thing feel free to give me better names so the question was will speak optimize optimizer query no it'll not change your sql query like the text of it what it will do is it'll try to optimize the way it sends it to the database um but sqladmin does not presume to know your intent when it comes to the to the sql when we talk about core uh when we talk about orm it's a bit more fuzzy but yeah that's also part of the sneaky abstraction thing we were talking about that if we try to hide like the uh sql code from the python code then it starts doing things that you don't expect and that that's part of the design principles of sql alchemy is that it doesn't try to hide anything it's not trying to be a a sql writer it's trying to be a sql yeah parsing engine let's call that cool anyone else then let's dive into the wonderful world of orms so the orm is actually where a lot of people get their starts in sql alchemy they're usually coming from something like django they're used to working in the orm there maybe now they're trying out some api language like flask or fast api and they're like where's my orm and then they find out that oh sql camera has an rm great in my opinion that's the wrong way to learn sqlcommy because again the orm does magic that's kind of what orms do they are mapping a python class to a sql row that's that's what an object relational mapper does that means that by its very nature there is magic going on to sort of convert between those two things which sounds the same thing but usually aren't so learning core first lets you always have that escape hatch of okay now i'm doing something i can just drop down into core and write it so it does it the way that i needed to do it and is one of the actually the core part in the pun features of sql alchemy that you can always switch in between because orm is just building on top of core so when would you use it uh so my rule of thumb is orms are great if you're talking about single rows so that's why it's very common in something like django or in an api you're doing request response get me this customer get me this purchase get me this uh blog post then it works great if you're only working with one road a time and manipulating that or a handful let's say then orms are great if you're working with analytics in any way you want to do counts sums aggregations of any kind core will probably be better for your use case because there's also one caveat if you were to select 100 000 rows and then convert them into python objects that's not fast converting them into pi into c clock rows is a lot faster so just keep that in mind that you want to think about tens and not thousands when you're talking about orm so again the first thing we always need to do is to define our tables that's how sql alchemy knows what's what it's got to work with so the way we do that is actually fairly different because now we're talking about classes so the classic way and i'll show you this because every single sql option tutorial you see out there is going to be using this way until everyone migrates to new 2.0 style is to run this function which returns a class and then you will subclass this derived class and then write your your own class on top of that so remember this library was written way before we had type checkers before we had linters this does not make my pie happy it does not make a lot of linting libraries happy in fact some of them have coded in special exceptions for c clock me again because c block me so pervasive but in general we don't want to return a class or even not even an instance of a class a type of a class from a function right that looks odd but that's how it was done so that's you can still do that that's not going away so you you create your your base class here and then you subclass it you always have to give it a table name and that's going to be what the name of the table in the database is going to be so the rule of thumb here is in its in your database you probably want plurals you want customers uh in orem you usually you have one customer at a time so there you're one singular one customer right and the other the rule is that a orm class has to define a primary key you're not allowed to have a rm object does not have a primary key because that's how sql alchemy keeps track of the individual objects in its own internal representation so if you can't fulfill those two things then oms are not for you but yeah but that's that's basically how you do it and when you do that behind the scenes there is again this metadata object being created it's inside of this base class and this table gets registered but in 2.0 one of the nice new features is this that they revamped this registry system so now we can create a registry instead and then this registry can be used to decorate your class with dot maps and that sort of fits in more with how we think about these things these days right we have data classes we have adders uh function classes that use this pattern uh having a class decorator is not a crazy thing to do right also fits more in with a registry pattern in general but the same rules still apply you still need a table name you still need your primary key and in this case we just have a random address but it's just a class so i can add a nice wrapper to it that shows me sort of the key data and then you know that will be printed out the other nice thing that it does i remember remember i told you that om is just building on top of core it just auto creates the c clock record table in this under table attribute so it's right there you can you can go grab it and work with it and do whatever you need to do uh notice that we're not finding in inits so this is actually something that's because again this predates data classes and adders and all these things this is one of the first libraries that i had this i'll create and in it for you based on class variables so cyclone will automatically create an init that accepts these but you are free to define your own in it if you want to either restrict what you want to accept or if you want to do some you know post-processing of the input arguments before you create your for instance that's perfectly fine let's finish up our data models so we have we have our address we have let's have a purchase i have this table arcs here orms don't really work well in notebooks they're not meant for that so par oh sorry sql alchemy will complain that i'm going to be redefining these classes throughout the notebook uh because it messes with this internal registry so i have this extend existing just means that it's okay i know what i'm doing i'm opting out of this checking behavior it'll still complain with a warning so don't worry about that but just to let you know so just ignore this line this is for notebook purposes only but yeah so the definition is pretty much exactly the same i have a foreign key i use these columns i have for example a decimal the nice thing is i can add these high pins again we're writing modern python i can add tie pins to my attributes here clock me doesn't care about them yet there's actually a feature that's coming up that you can do type hinting to instead of writing these essay.com things but it's not a feature yet so right now it's just for my own my own sake it helps when we write our code we can also use more exotic types like we can use an enum so for the customer these customers have a status they can be gold silver bronze so then we can set the status to be of type enum and then we get to the cool part about orms which is relationships which is to me the killer feature so notice here that i define two relationships address so a customer has an address and purchases a customer has many purchases so what does that actually mean so i still have my database up and running if you don't then you have to recreate it so we have a new keyword here and again oms are magical they will create and run sql on your behalf that you have no control over per se so when you're getting used to working with the orm you can turn on this echo equals true then sql alchemy will print out every sql statement it is writing and running so that gives you that extra level of control when you're developing that's okay it is actually doing what i think it's doing because especially with orms they're going to be doing a lot more calls than you think they're doing and then i'll do the same thing notice that on the map registry that's where my metadata lives now so i can just access it through there and here's our first example of all the stuff that clapping actually does for me some highlights is that actually checks do these tables actually exist already if not skip them in a cross database agnostic way and then actually runs all the creates and creates all the primary keys and foreign keys and all those references as we saw before and now i'm ready to actually create some data so you just create an instance it's just instances of class i have john jane here they're not i haven't again i haven't talked to the database yet they're just a class with attributes dot name and dot status everything you would expect from a python class nothing up my sleeves so the way you talk to your database in rm is with a thing called session you're no longer working with a direct connection object you're working with a session because the section adds some extra abstractions on top like keeping track of what is already loaded into memory what is not loading primary keys into memory so it can access various data so we need the session thing instead but the pattern looks very similar in the sense that we create an instance of session give it the engine in a width block and that's we're just going to call session.execute instead of con.execute so pretty close but not quite so in the session we add these objects and then c-clock we make sure that uh now it knows to keep track of them again i still have to actively commit it's still an opt-in commit so if i commit that you can see we get the inserts and notice speaking of optimizing sql queries i have from this i get one insert that is parametrized i'm not getting two insert statements and that's that unit of work in in action because the session knows oh i have two of these two of these things that aren't committed then i can optimize what i'm sending to the database and write the sql in a way that optimizes for that inserts so instead of sending 100 it will send one insert with many parameters so now we can add an address and we can do that in many ways but one way is to just assign it to the attributes so now john has an address so now if i re-add john and commit that so you clock me will notice hey john now has an address he didn't have one before and doesn't exist so let's insert into addresses this new address and then we select again to refresh the session because we update we have to make sure that john himself hasn't changed and then we update him to insert the address id of the newly created address so if we jump into the database and look at what actually happened so this is the magic part of the ograms right having to keep track of relationships across tables is a pain so i have my address and then i have john with address id of one that's pretty cool notice that jane here she doesn't have an address yet so she's just null so john goes shopping and he buys a magic potion so when i create purchase because i set this relationship up uh if i go to purchase notice i didn't set one on purchase but yeah the only thing about these sql queries is they get pretty long you know when you're doing the echo we were down here somewhere but this still works so why do you think that works that's a bit odd well if we look at our customer here you notice i set a relationship to the address table backref equals customer so backref is back reference that means that what also does is it registers on the address class that hey you have a relationship to customer you know about until now but now you do which means that it adds that attributes to the address class or to the purchaser class and any customer so you can do it manually you can explicitly set a relationship on each side and then do back populates which will say like there is a matching relationship here or you can set the back graph here and then it will just add that attribute for you so that's more up to your encoding style explicit versus implicit and all those fun things but now if i add the potion we see the same thing we select the customer so we got a fresh copy of the customer we insert into the purchases so this is going on and then we insert it with the user id but again that's the caveat right it's doing some magic for me but i wouldn't be able to guess how many sql statements that simple operation would have done if i wasn't quite very familiar with c locally so in this case i did two it could have done lots more if there were if there was more data so caveat emptor always make sure that you know what your orem is doing we can also just create a purchase so we have our magic hat here then i can again connect john to the session because if john's not in the session then sql acme doesn't know how to look them up to make sure we get the right data and it'll just complain and tell you hey i'm not in a session so please add me so when i have john then i can append him to my purchases attributes and then because equal me just rep represents these relationships as a list i can read and then i can commit and again we're generating a bunch of sql statements three in this case but it also does some caching which is nice to know that's a new feature in 2.0 it does more aggressive caching of your queries so the compilation of the sql statements gets cached by sql alchemy and that's what it's telling you here that it it's already seen in the sql so it doesn't actually need to uh to recompile it so now if i go back into my purchases i have two purchases and they are both are mapped to the correct user id so this is this was my mind blown moment when i started working with clockme because doing this by hand sucks that is like the worst part of writing sql for some kind of api is keeping track of all these related objects and making sure that they're in sync at all times so if it takes a few more sql calls to to make that happen by magic then i'm perfectly comfortable with that so we have some data and how do you actually now get that data back so the big thing that happened to 2.0 was a realignment if you're looking at pre 2.0 then you would have different syntaxes for selecting depending on whether it was orm or whether it was core which meant that every time you googled how do i do this sequel in sql alchemy you always have to make sure that you're either looking at the orm version or the core version which was really annoying in 2.0 it's been aligned now we're using the same essay that selects whether or not it's in orm or whether or not it's in core and that's fantastic only one api to learn right so in the end you just say essay to select customer it's the same thing as if it was a table in core uh filter by is just a nice syntax instead of saying dot where name equals equals jane you can do filter by keyword arguments value and just as the same so it's a convenience method and it creates the the correct sql so and again the execution is almost the same we're just using a session instead of the connection but we do session execute one or none and we get our jane back so especially one or none is something you're going to be using a lot more if you're using the orm because then you're typically in a context where either the customer exists or he or he does not so you're fine either way but you just need to know but if there are two customers with the same value then you probably did something wrong so that's a nice level of safety you get there so and we have our customer chain but wait that looks a bit odd what happened here so again things changed a bit in 2.0 and if this was 1.4 it would not look like this you'd get your chain back and everyone wouldn't be happy but now we're aligning our syntax across core and orm and in core when you do select you get back a row object and you have to access each value in it exact same thing for orm so if i do jane 0 then i get jane back out i actually don't remember is a name customer yeah so that basically get back your result a row that has one one column in it and that column is called customer and it has j inside so obviously that's a bit clunky so we introduced this thing called scalars so scalar was actually a thing back in the day as well but it wasn't necessary for rm now it is so scalar just says i want to whatever is in the first row in the first column that's the value i want and if you're using multiple then it's just whatever's in the first column that's the value that i want so you tack on scalars to your executes do one or none and now we get back what we would expect so because this is such a common pattern in 2.0 we have a bunch of these helpers which do it in a single method call instead so you see scalar one or none scalar all scalar versions of all the various operations but just know that just it just means adding the scalars modifier to the previous results access so one more way because obviously in python we like to have multiple ways if you know the customer or the primary key then you can use a sort of shortcut syntax which lets the session optimize the query uh because it checks do i have this already uh can i reuse something or do i have to go fetch it so if you do session.get on the table and pass the primary key then i'll do this optimization to to get out the customer and we get the same customer back so now that we have our customers we can start asking questions about the related attributes we set up these relationships and we probably did it because we care about them so let's put john back into the session and then let's ask about his purchases and his address so we got a bunch of sql being generated but all i did was just access an attribute so what's up with that and if i do this with a regular attribute there's no sql being emitted at all so this comes back into what relationships uh actually are and why you have to be a bit careful you see we have here one selecting the customers we have selected purchases and then we select the addresses so these two dots these three actually created three sql statements even though you're just doing attribute lookup so this idea of relationship loading is pretty important if you want to really take advantage of relationships so by default sql alchemy orms are lazy if you're fetching a customer you're only getting the rows from the customer table because joins are can be expensive so sql acme doesn't want to presume that you automatically want to get the join with all the addresses or pushes imagine like if you're a twitter and you get one you know get all your posts or it's a massive join right so by default it's lazy because that's the safest if you don't know what's going on though it becomes quite dangerous because then you're doing attribute looks up every attribute lookups everywhere not knowing that you're constantly sending sql statements to the database which will be fine when you're in dev and then you hit production all of a sudden your scale increases by 10 folds 100 folds and your sql query is increased by a thousand volts so to get around this you can explicitly in a query you can explicitly say i want this to be done in a different way so when i select customer here i i can pass this options and i can say that the address relationship should be done with a joined load and the purchases relationship should then with it select in load where customers name equals to john so if we look at this now so the if i get the customer here let's see if you can find that then you see i do a left outer join with addresses so that's this is this join load so then in one sql statement it's doing a left join with the relationship table and getting the all those in because obviously we want to have all the customers but we also don't have an address that's perfectly legitimate query the other the other option is to do a select in so there we can select the purchases where purchase a user id in primary key so there you're reducing it to two selected statements one to get the customer and then another one to select all the purchases where the purchase id is in this part this is a custom id so again it comes down to two flavors and uh it's more up to you uh sort of exactly how it should be done because you're the one who knows the database again sequel oximeter's philosophy is we don't leak we don't hide away the sql you are still in charge of the database you know best about how the database is structured we're not going to make all these assumptions for you we can also set these up from the start so when i define my relationship i can also say how should this by default be done so i can say lazy equals joined we'll do the join load or laser equals selection select in which you'll lose the selected load so you can predefine that if you know that every time you want that and then you can opt into other behaviors depending on your query so i'm going to skip over this exercise so you're free to do this for yourself i just want to talk about what some of the nice things that orams give you on top of relationships which mainly are that orms are just classes and classes are first class contracts in python it gives you a lot of nice features for example you can create a mix in so let's say you want to have two columns on every table that says when was this created and when was the last updated normally you'd have to do that by hand but you can just specify mixin so if you haven't seen mixing before it's just a name for a pattern where you're not supposed to override it it doesn't have behavior it just adds some extra stuff to your class and then if i list my columns here you can see i now have two more created ads and some over here should be the yeah there last updated and created that gets automatically added to this class [Music] so the other nice thing is we can define a class method so this is actually a pretty powerful construct in python how you define alternative constructors especially if you're you know doing serialization you get some json you want to convert it into your class the easiest way to do that is to create a class method that defines okay from dict i'll take addict and then creates the class based on that so you can control that behavior so this is the warning i was talking about that i'm redefining the class which you should not be doing for demo purposes only so now i can create a user object from dictionary and sort of control that behavior we also have properties like is admin i can of course manually write the code to check if this user is an admin or not or i can add a property which makes it look like it's an attribute so now i can do is admin false the nice thing is that sql alchemy actually lets you hook into this behavior in in python so we have this thing called a hybrid property so hybrid property just means that we tell c blocking hey this thing you can actually use it in the sql statement as well so instead of defining a property we define a hybrid property and then we create all that and we get a user we add that user and now i can do in my sql statements notice i'm using the class because it's a class property and not an instance property is admin and that will compile into or uses that role equals role so now i have a nice class property that i can use both after i get my instance and in my sql statements we can also verify those mixed in while we're at it so last updated 9 23 26 and 9 23 50. so those are actually working as intended as well sometimes the logic doesn't matter as easily in this case property was a simple equals that gets over overloaded in sql alchemy and gives you as a where clause often is not that neat so let's say for example that we have that the role can be one of many so we're doing it in that doesn't translate to sequel to me so we need to specify the sql that gets run in the sql world to match the result from the python world the way you do that is you specify your hyper property this is the python code and then you can do another one it has to be named the same and then you you say the proper name dot expression that tells people alchemy this is where the sql expression you should use comes from and it's on the class level so you get a class and i can say class that role in this is the way you say usually create an in statements in a c locate this list so now i have my user i have a property that i can access on the instance or i can use the is validated so just notice that because we're using a bit of cpu magic it can't quite show you what's going to happen because it needs to do some extra processing of the query parameters that come in so which is why you get this funky post-compiled world but it does work and we get back jade who was validated so that's how you can do that so we only have five minutes left uh so i just want quickly to mention hybrid methods basically the same as a hyper property it just works for methods instead so let's say we have our purchase here we want to define the ability to calculate an roi on the on our user here sorry so basically take the sum of all those purchases and divide it by the total cost and that'll give us an roi on this customer so i need to have some information in order to calculate this which is why i can't be property uh i need the cost so i execute oh boom boom i'm sorry there's some duplication here don't write your demos late at night people uh here we go so here we have a jnr user with the purchase of zero i can also have a plain jane regular method here uh purchase which just adds to the purchase cost notice that you can do nice things like this where you can abstract away some logic you can pass into the active session and then actually work with the instance directly on the on the method so this case i update the purchases with the item cost and then i persist that into the database and then this roi method down here [Music] so now we have some purchases so if we looked at here notice it's actually selecting from the database the purchases and then i can actually calculate the roi this is on the instance so it's not doing any sql but i can then use it in a where statements so where roi is greater than one for example and that translates it into the correct statements so i can execute that and then i get back my chain and then her roi just to show that it was working is greater than one and with that i will close this session so thank you very much for your time so we have uh three minutes before alisa i get kicked out you're free to say you can ask some questions on here or otherwise just meet me outside i'll be here all weekend so good timing so yeah we got a question over there with things like data glasses and things like that but this one's keep some extra functionality like a good wrapper method that is there to combine to choose somehow uh yeah actually uh that's one of the imperatives for changing the way of registering uh classes was explicitly to support data class and adders so you can actually have your data class and then tell sql alchemy this is the class that i want you to use as sort of the serialization class and then it just works peanut butter and chocolate so i would refer to the docs considering we have two minutes um but if you do see glacomy so the only it sounds amazing but it is a bit more verbose because obviously it has to do a bit more work oh that's the one i wanted see and i will plug the c clock documentation they are fantastic um they do a lot but a lot of work on it so this whole section here is how you can do exactly that so basically you have to redefine everything twice because you need the c blocking representation of the table and your data class representation so it's not as pretty but you don't have to do it in line like you could have your tables definition somewhere else and then you know just say set the table equal to it because this is the piece of magic that the mapper also does is to generate this for you it's um oh we have one all the way back over there excellent question so you're asking about the data immigration database migration tool for student uh so the same guy who wrote c block me also wrote alembic uh so basically alembic lets you is able to read all your sql community definitions and then automatically create a migration based on that by introspecting the target database your models and creating a diff of the two and then writing the ddl to create the difference between the two then also register when you apply it it also registers in the database hey version blah blah blah is the current one so it can always keep them in sync knowing what version your database is in so then you can do like git style upgrade downgrade between versions of your database so so check that out that's uh the current gold standard for or at least for c locally thank you very much [Applause]\", metadata={'url': 'https://www.youtube.com/watch?v=X4-hu3vZAOg&list=PLGVZCDnMOq0qT0MXnci7VBSF-U-0WaQ-w&index=1', 'title': 'Anders Bogsnes - SQLAlchemy and You - Making SQL the Best Thing Since Sliced Bread - YouTube'})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_with_title[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anders Bogsnes - SQLAlchemy and You - Making SQL the Best Thing Since Sliced Bread - YouTube\n",
      "Kajanan Sangaralingam and Anindya Datta - Feature Engineering Made Simple | PyData London 2022 - YouTube\n",
      "Raymond Hettinger: Numerical Marvels Inside Python - Keynote | PyData Tel Aviv 2022 - YouTube\n",
      "Dina Bavli - Life, Death, and Shopping | PyData Global 2022 - YouTube\n",
      "Srikanth - Use pandas in tidy style | PyData Global 2022 - YouTube\n",
      "Aadit Kapoor - Utilizing Word Embeddings and Gradient Boosting | PyData Global 2022 - YouTube\n",
      "Introduction to data apps with Panel, Pydata Copenhagen, March 2022 - YouTube\n"
     ]
    }
   ],
   "source": [
    "for doc in docs_with_title:\n",
    "    title = doc.metadata.get(\"title\")\n",
    "    if title:\n",
    "        print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "\n",
    "tokens = []\n",
    "for document in documents:\n",
    "    tokens.append(num_tokens_from_string(document.page_content, \"gpt-3.5-turbo\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimalna ilość tokenów wśród dokumentów: 1385\n",
      "\n",
      "Maksmylana ilość tokenów wśród dokumentów: 15210\n",
      "\n",
      "Łączna ilość tokenów we wszystkich dokumentów: 44887\n"
     ]
    }
   ],
   "source": [
    "print(f\"Minimalna ilość tokenów wśród dokumentów: {min(tokens)}\\n\")\n",
    "print(f\"Maksmylana ilość tokenów wśród dokumentów: {max(tokens)}\\n\")\n",
    "print(f\"Łączna ilość tokenów we wszystkich dokumentów: {sum(tokens)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"graphics\\pricing.png\" width=\"800\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"\", \" \"],\n",
    "    chunk_size=700,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pewnego szarego poranka na Dolinę Muminków spadł pierwszy śnieg. Padał miękko i cicho - w parę godzin wszystko było białe.Muminek stał na schodkach pr',\n",
       " 'stał na schodkach przed domem patrząc, jak dolina okrywa się zimową kołdrą.- Dziś wieczorem - myślał sobie - ułożymy się do długiego zimowego snu. (Ws',\n",
       " 'go zimowego snu. (Wszystkie trolle Muminki układają się do snu zimowego gdzieś koło listopada. Bardzo to rozsądnie ze strony każdego, kto nie lubi zim',\n",
       " 'go, kto nie lubi zimna I zimowych ciemności). Potem Muminek zamknął za sobą drzwi i poszedł do swojej mamy.- Śnieg przyszedł! - powiedział.- Wiem - od',\n",
       " 'wiedział.- Wiem - odpowiedziała Mama Muminka. - Macie już wszyscy w łóżeczkach najcieplejsze kołdry. Ty będziesz spał w pokoiku na poddaszu po stronie',\n",
       " 'poddaszu po stronie zachodniej razem z Ryjkiem.- Ale Ryjek tak okropnie chrapie - powiedział Muminek. - Czy nie mógłbym zamiast z nim spać z Włóczyki',\n",
       " 'nim spać z Włóczykijem?- Jak chcesz - odpowiedziała Mama Muminka. - Ryjek może spać w pokoiku na poddaszu od strony wschodniej.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Pewnego szarego poranka na Dolinę Muminków spadł pierwszy śnieg. Padał miękko i cicho - w parę godzin wszystko było białe.\\\n",
    "Muminek stał na schodkach przed domem patrząc, jak dolina okrywa się zimową kołdrą.\\\n",
    "- Dziś wieczorem - myślał sobie - ułożymy się do długiego zimowego snu. \\\n",
    "(Wszystkie trolle Muminki układają się do snu zimowego gdzieś koło listopada. \\\n",
    "Bardzo to rozsądnie ze strony każdego, kto nie lubi zimna I zimowych ciemności). \\\n",
    "Potem Muminek zamknął za sobą drzwi i poszedł do swojej mamy.\\\n",
    "- Śnieg przyszedł! - powiedział.\\\n",
    "- Wiem - odpowiedziała Mama Muminka. - Macie już wszyscy w łóżeczkach najcieplejsze kołdry. \\\n",
    "Ty będziesz spał w pokoiku na poddaszu po stronie zachodniej razem z Ryjkiem.\\\n",
    "- Ale Ryjek tak okropnie chrapie - powiedział Muminek. - Czy nie mógłbym zamiast z nim spać z Włóczykijem?\\\n",
    "- Jak chcesz - odpowiedziała Mama Muminka. - Ryjek może spać w pokoiku na poddaszu od strony wschodniej.\"\n",
    "\n",
    "text_splitter_muminki = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"\", \" \"],\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "\n",
    "chunks = text_splitter_muminki.split_text(text)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content=\"thank you very much for being my guinea pigs for uh for today it's always fun to be the first uh first of a pi data london session so as mentioned my name is anders i am norwegian i live in copenhagen and have lived lots of different places i have a background in japanese business actually so i've had a lot of hats i've currently i am as mentioned the head of the python enablement team at modis management so my job is to make sure that we have all the python tools we need build up python infrastructure do courses trainings workshops sort of be internal consultants for all the teams at uds management so you can imagine a lot of quants a lot of trading i'm sure you all you londoners know all a\" metadata={'id': 'bd9aa202dfe6-0', 'source': 'https://www.youtube.com/watch?v=X4-hu3vZAOg&list=PLGVZCDnMOq0qT0MXnci7VBSF-U-0WaQ-w&index=1', 'title': 'Anders Bogsnes - SQLAlchemy and You - Making SQL the Best Thing Since Sliced Bread - YouTube'}\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "m = hashlib.md5()\n",
    "\n",
    "data = []\n",
    "for doc in docs_with_title:\n",
    "    url = doc.metadata[\"url\"]\n",
    "    title = doc.metadata[\"title\"]\n",
    "    m.update(url.encode(\"utf-8\"))\n",
    "    video_id = m.hexdigest()[:12]\n",
    "    chunks = text_splitter.split_text(doc.page_content)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        data.append(\n",
    "            Document(\n",
    "                page_content=str(chunk),\n",
    "                metadata={\"id\": f\"{video_id}-{i}\", \"source\": url, \"title\": title},\n",
    "            )\n",
    "        )\n",
    "\n",
    "print(data[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding - osadzanie słów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings_openai = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zapis danych w bazie wektorowej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Qdrant\n",
    "\n",
    "vectorstore = Qdrant.from_documents(\n",
    "    data, embeddings_openai, path=\"pydata_db\", collection_name=\"vectorstore\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zdefiniowanie modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat_07 = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=800,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_0 = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    temperature=0,\n",
    "    max_tokens=800,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zdefiniowanie łańcucha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "chat_qa_07 = RetrievalQA.from_chain_type(\n",
    "    llm=chat_07,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_qa = RetrievalQA.from_chain_type(\n",
    "    llm=chat_0,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Odpytywanie modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = input(\"What kind of topic would you like to explore?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature selection\n"
     ]
    }
   ],
   "source": [
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Feature selection is the process of selecting a subset of relevant features (variables, predictors) for use in model construction. It is an important step in the machine learning pipeline as it helps to reduce the complexity of the model, improve its accuracy, and prevent overfitting. Feature selection involves generating a set of candidate features, pruning the feature set using a set of checks, and selecting the final set of features that will power the model. The candidate feature set can be generated in various ways, including using a feature explorer, generating custom features, and applying transformations to existing features. The final feature set is selected based on its quality, relevance, and ability to improve model performance.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_qa_07.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The process of feature selection involves selecting a subset of relevant features or variables to use in building a model. This helps to improve the accuracy and efficiency of the model, as it reduces the number of irrelevant or redundant features that can negatively impact the model's performance. The first step in feature selection is to generate a set of candidate features, which can be done through various methods such as using a feature explorer, generating custom features, or using transformations. The candidate feature set is then pruned using a set of checks, such as null detection, missing value detection, outlier detection, and biasedness detection. The final selected features are used to power the model.\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_qa_07.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The process of feature selection involves selecting a subset of relevant features from a larger set of candidate features that will be used to build a predictive model. The first step in feature selection is to generate a set of candidate features, which can be done using various methods such as the feature explorer, custom feature generation, and transformations. Once the candidate feature set is generated, it is pruned using a set of checks such as null detection, invalid entry detection, missing value detection, outlier detection, and biasedness detection. The final set of selected features will be used to build the predictive model.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The process of feature selection involves selecting a subset of relevant features from a larger set of candidate features that will be used to build a predictive model. The first step in feature selection is to generate a set of candidate features, which can be done using various methods such as the feature explorer, custom feature generation, and transformations. Once the candidate feature set is generated, it is pruned using a set of checks such as null detection, invalid entry detection, missing value detection, outlier detection, and biasedness detection. The final set of selected features should be those that are most relevant to the problem being solved and have the highest predictive power.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_qa.run(query)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usprawnienie łańcucha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "chat_template = \"\"\"\n",
    "You are an AI Assistant providing information about PyData talks available on YouTube. \\\n",
    "You specialize in Python, machine learning, artificial intelligence and data visualization. \\\n",
    "Your audience consists of Python programmers interested in these topics. \\\n",
    "If a question is related to the PyData talks, you provide a relevant answer. \\\n",
    "If the question is about other topics not covered in the videos, kindly respond that it was not a topic \\\n",
    "discussed in any of the lectures on PyData. Do not make up answers.\n",
    "\n",
    "Add what is the title of the document that were used for answer preparation.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Assistant: \"\"\"\n",
    "\n",
    "chat_prompt = PromptTemplate(\n",
    "    template=chat_template, input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_qa = RetrievalQA.from_chain_type(\n",
    "    llm=chat_0,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    chain_type_kwargs={\"prompt\": chat_prompt},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = input(\"What topic would you like to explore?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is there any way that I can improve my pandas experience?\n"
     ]
    }
   ],
   "source": [
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, you can improve your pandas experience by using Tidy Pandas, which offers a simplified index, consistent verbs, and a unified interface for aggregation and assigning your columns across groups. Tidy Pandas also offers an accessor for pandas data frames, which allows you to directly call Tidy Pandas methods on your data frames. Tidy Pandas is available on PyPI and has been actively maintained since its first release in April 2022.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = \"\"\"\n",
    "You are an AI Assistant providing information about PyData talks available on YouTube. \\\n",
    "You specialize in Python, machine learning, artificial intelligence and data visualization. \\\n",
    "Your audience consists of Python programmers interested in these topics. \\\n",
    "If a question is related to the PyData talks, you provide a relevant answer. \\\n",
    "If the question is about other topics not covered in the videos, kindly respond that it was not a topic \\\n",
    "discussed in any of the lectures on PyData. Do not make up answers.\n",
    "\n",
    "Add what is the source of your knowledge.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Assistant: \"\"\"\n",
    "\n",
    "chat_prompt = PromptTemplate(\n",
    "    template=chat_template, input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_qa = RetrievalQA.from_chain_type(\n",
    "    llm=chat_0,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    chain_type_kwargs={\"prompt\": chat_prompt},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, there are ways to improve your pandas experience. One way is to use Tidy Pandas, which offers a simplified index, consistent verbs, and a unified interface for aggregation and assigning your columns across groups. Tidy Pandas also offers an accessor for pandas data frames, which allows you to directly call Tidy Pandas methods on your data frames. Tidy Pandas is available on PyPI and has a GitHub repo. Source: PyData talks on YouTube.'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_qa.run(\"is there any way that I can improve my pandas experience?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, I'm not sure which previous question you are referring to. Could you please provide more context or clarify your question? My knowledge is based on PyData talks available on YouTube, specifically related to Python, machine learning, artificial intelligence, and data visualization.\""
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_qa.run(\"Can you tell me what talk exactly is the basis of the answer for your previous question?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zwracanie źródła informacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_qa = RetrievalQA.from_chain_type(\n",
    "    llm=chat_0,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    chain_type_kwargs={\"prompt\": chat_prompt},\n",
    "    return_source_documents=True,\n",
    ")\n",
    "\n",
    "result = chat_qa({\"query\": \"is there any way that I can improve my pandas experience?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'is there any way that I can improve my pandas experience?', 'result': 'Yes, you can improve your pandas experience by using Tidy Pandas, which offers a simplified index, consistent verbs, and a unified interface for aggregation and assigning your columns across groups. It also offers an accessor for pandas data frames, making it easier to use Tidy Pandas methods directly on your data frames. Tidy Pandas is available on PyPI and has been actively maintained since its first release in April 2022.', 'source_documents': [Document(page_content=\"alysis right and you see another group by an aggregation probably on line seven and eight that that lets you and you got to again rename what you have done in the aggregation so when you do this and you keep constantly printing your data and takes a lot of time and takes you away from your thought process so we believe uh tidy pandas really should help us in that regard and tidy pointers uh really works towards a lot of consistency so if you see the first three statements and try to pointers so all three of them return your tidy data frame depending on the the method that's called okay so these things could be really painful when you're doing the analysis so this really matters so tidy panda\", metadata={'id': 'bf69d53acddf-7', 'source': 'https://www.youtube.com/watch?v=AMJEnkA0YrE&list=PLGVZCDnMOq0qgYUt0yn7F80wmzCnj2dEq&index=12', 'title': 'Srikanth - Use pandas in tidy style | PyData Global 2022 - YouTube'}), Document(page_content=\"ti-intention now I'm wrangling this could be a real headache right this is one of the points where we feel that a lot of data analysis can and should be done without using indexes and mighty indexes should something we should avoid at all costs right so here is an example where 3D printers offers an accessor for the pandas data frames so this is for those guys who say I am working with my pandas data frames tell me if there's something that I could use Quick right so here's the way so import tidy accessor sdp so once you've done that that on your pandas data frames you could directly call pandas Thai dependence methods so if you recall this example from a first slide so I'm actually calling\", metadata={'id': 'bf69d53acddf-9', 'source': 'https://www.youtube.com/watch?v=AMJEnkA0YrE&list=PLGVZCDnMOq0qgYUt0yn7F80wmzCnj2dEq&index=12', 'title': 'Srikanth - Use pandas in tidy style | PyData Global 2022 - YouTube'}), Document(page_content=\"type and uh tidy pandas is available on poipi there's a GitHub repo this website uh will be very happy to take your suggestions issues and the first release happened on April 2022 this year and we've been having nine releases since then and uh a very active project that's been maintained and we want to maintain this very good long time yeah thank you that's it from my side\", metadata={'id': 'bf69d53acddf-12', 'source': 'https://www.youtube.com/watch?v=AMJEnkA0YrE&list=PLGVZCDnMOq0qgYUt0yn7F80wmzCnj2dEq&index=12', 'title': 'Srikanth - Use pandas in tidy style | PyData Global 2022 - YouTube'}), Document(page_content=\"lide which basically spoke about what are the advantages I'll go back to this again uh because this kind of tells us what we're trying to do right we have a simplified index we have consistent verbs we have a unified interface for aggregation assigning your columns across groups right we don't make a copy between uh tidy frame and pandas and if you prefer not to we offer an accessor so all of these things should make the data analysis very elegant right so with this we saw a few working samples where we really saw examples where things become very compact right and all the underlying code is written in Panda so it is as fast as pandas can get type and uh tidy pandas is available on poipi the\", metadata={'id': 'bf69d53acddf-11', 'source': 'https://www.youtube.com/watch?v=AMJEnkA0YrE&list=PLGVZCDnMOq0qgYUt0yn7F80wmzCnj2dEq&index=12', 'title': 'Srikanth - Use pandas in tidy style | PyData Global 2022 - YouTube'})]}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.youtube.com/watch?v=AMJEnkA0YrE&list=PLGVZCDnMOq0qgYUt0yn7F80wmzCnj2dEq&index=12', 'https://www.youtube.com/watch?v=AMJEnkA0YrE&list=PLGVZCDnMOq0qgYUt0yn7F80wmzCnj2dEq&index=12', 'https://www.youtube.com/watch?v=AMJEnkA0YrE&list=PLGVZCDnMOq0qgYUt0yn7F80wmzCnj2dEq&index=12', 'https://www.youtube.com/watch?v=AMJEnkA0YrE&list=PLGVZCDnMOq0qgYUt0yn7F80wmzCnj2dEq&index=12']\n"
     ]
    }
   ],
   "source": [
    "sources = [doc.metadata[\"source\"] for doc in result[\"source_documents\"]]\n",
    "print(sources)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternatywa - wyszukiwanie podobieństw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Document(page_content=\"alysis right and you see another group by an aggregation probably on line seven and eight that that lets you and you got to again rename what you have done in the aggregation so when you do this and you keep constantly printing your data and takes a lot of time and takes you away from your thought process so we believe uh tidy pandas really should help us in that regard and tidy pointers uh really works towards a lot of consistency so if you see the first three statements and try to pointers so all three of them return your tidy data frame depending on the the method that's called okay so these things could be really painful when you're doing the analysis so this really matters so tidy panda\", metadata={'id': 'bf69d53acddf-7', 'source': 'https://www.youtube.com/watch?v=AMJEnkA0YrE&list=PLGVZCDnMOq0qgYUt0yn7F80wmzCnj2dEq&index=12', 'title': 'Srikanth - Use pandas in tidy style | PyData Global 2022 - YouTube'}), 0.8097646888396879)\n"
     ]
    }
   ],
   "source": [
    "found_docs = vectorstore.similarity_search_with_score(\n",
    "    \"is there any way that I can improve my pandas experience?\"\n",
    ")\n",
    "document, score = found_docs[0]\n",
    "\n",
    "print(found_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.youtube.com/watch?v=AMJEnkA0YrE&list=PLGVZCDnMOq0qgYUt0yn7F80wmzCnj2dEq&index=12\n",
      "Score: 0.8097646888396879\n"
     ]
    }
   ],
   "source": [
    "if sources[0] == document.metadata[\"source\"]:\n",
    "    print(document.metadata[\"source\"])\n",
    "else:\n",
    "    print(\"Sources does not match\")\n",
    "\n",
    "print(f\"Score: {score}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Podsumowanie video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"hi there thanks for taking the time in this video i'll give you a recap about a talk i recently gave at pi data copenhagen so i was invited to talk about an introduction to data apps with panel and since the event was not recorded i'll try to recap some of the things i did so first of all i gave an introduction to awesome panel which is the site i've created so panel is a framework for creating powerful data apps and for interactive data exploration in python and here at awesomepanel.org you can find inspiration you can find out if panel is something for you i believe it could be at least panel is something for me and i wrote a blog post about it you can check it out one of the amazing things about panel is that it really works in the notebook and your editor so both in both places and this is like a very unique value proposition panel is open source for free and it's part of a broader family of systems that try to unite the pi data or pybis tools so at awesome panel you can also find a few galleries the app gallery is really all the applications that you see here in the sidebar let's check it out and the community gallery is more like the awesome list so this is the awesome panel gallery like this you can go in here find a lot of examples for inspiration you can find the code so a link to the code there might also be a video or other kinds of links in here for example we have up here so if you click this one the bootstrap dashboard for example you will go to a page where yeah you can just see the small example here and again we have the code here and you can just open it on github and take a look at how it's created yeah we had the awesome list or the community gallery as well so here you can get inspiration you can click and go to the resource and there might be additional links like github links for example this the github link here is the repository of this application and so on but what i really want to demonstrate in this video is the application that we were building at pi there 22 so i just want to showcase how easy it is to build applications with panel so in order to use panel we need to import spn and now you can see i've started working here in vs code and that's because that's my preference i prefer working in this code but you can actually also work just as efficiently in a notebook if that's what you prefer so we can search here panel jugular panel preview and then we will see here that yeah we can get a nice efficient development environment as well in a notebook because we have this jupyter panel preview that reloads this application over here every time we save the notebook so you get instant feedback and that's really important for your development experience but let's continue over here so the first thing you have to do every time is sort of import some of the things you need for panel especially in the notebook this is required and then we can make a basic hello world so hello hi that would be our message and here we really almost have a small application we can start up the server by writing panel serve the name of the script and then we can give it like an auto reload and show show will make it show up over here you can see it's blank now and that's because we need to mark the things we want to serve on our server as serverable and if the screen is blank here the the code and server is not connected yet so we need to reload again now you can see we get it up here and the nice thing is that piano panel sort of recognizes stuff so this thing it now recognizes as a markdown string and it converts it nicely over here and again you see as soon as we save it's updated this is not the most beautiful application or anything so what we can do is we can add a template like this and now we can update the template so we can refer to the pn.template and then param.update so we want to update some parameters like the site is something for pi data copenhagen 2022 or just pi data copenhagen i guess and the title is something like an introduction to data apps and you can see you get a nice thing here and what we could do furthermore is that we could actually add a sidebar so we could have some settings and make them servable so now we have like two cards here but i want to show them in the sidebar so i add area equal sidebar and then you get this now we could say the side width is too wide so we could set it to 200 pixels for example that's nice and we might also yeah that's fine okay so i want to show you data apps and data apps are normally interactive so they have widgets so the first thing we can do is we can say okay we probably have a some function or model so we can define a functional model and that could just return this one so now we get an error and that's because we we don't display anything yet so now we actually have our model and it returns the string and we see it over here now you can see it doesn't stretch nicely and that's because we need to say that we don't want fixed sizing on any panel we actually want a sizing mode called stretch width like that this model is not so interesting let's just give it a feature or give it a an argument so what about giving an argument five and saying now we want to do something like this so now you can see here it gets something like this one thing one problem here is that hmm count minus three and so on that's not so useful so we actually want to control the widget here so we can define the widget for the counts and panel has a lot of widgets including an end slider and here we could set the value equals five and star equals zero and equals five for example and make this servable so now we add an extra slider again we want to show it in the sidebar like this and we want to have the settings on top and now we need to connect or bind our model and now we did so we do that using pn.binds the model and then the count widget so we need to find the model the argument called count that to the count we did and then we get a new interactive model activity out we can call that i model so now we have a custom widget over here let's give it a name as well like name equals count and you can see here now it's called count let's just check it out so we also have the dark theme that's nice the magic of the panel function is also it can like understand other kinds of objects like data frames so it can understand or interpret a lot of pi data's pi data's objects so import pandas spd so now we could have pd data frame instead and then say x equals [Music] it looks like we have something here it should be a predict like this and now it's blank again then you need to refresh it like this so now you get a nice table and we can even say to the panel here that please please don't display the index nice and now you can see here and we can also here loading in decatur equals true so if we want a loading indicator we can get it so that every time we reload here we get the loading indicator and that's useful if you're sort of re-running the model model or the function is slow but let's just check this one out another thing you often want the parts one plotting library or framework you can use this hp plot so this h3plot.pandas when we import that then it adds the dot hvplot interactive methods to pd data frame to the pandas data frame so now we can use that to say h3 plots let's see what happens so now we get plots and we can say color equals let's just find an accent let's make that accent ah it's because we put it inside the data frame we should we should put it inside like that so now we get the orange color here and we would actually like to have it for a template as well let's just run black to relay out this so that we can see what's going on like this [Music] so we can add a accent base color and a header background so now it looks like this and let's also make it a little bit serious right so this is what we're looking at and yeah maybe we could even add logo okay we need some images and put it in a specific panel so that pinstrip t1 like this let's see how it looks so yeah now we didn't include it so we need to make it server and then i get some error down here so i need to like reload it a few times and i get something up here it's not really showing so let's say sizing mode let's see what happens it's not really showing anything it's it might be bad let's just copy this one okay let's see okay it downloaded it that was nice and i want to have it in the sidebar instead like that yes so now we have a nice application here you can see it updates when i drag the slider i showed you we could use pandas data frames we can use h3 plot we can also for example use plotting let's use partly express spx and then we need to change a few things here so first of all our data the data frame here then we need to say that block is px dot for example line and then we want the data here let's just see what happens so now it's downloading yes looks nice need something more like the ability to set the color and i don't color the screen sequence it probably is like this again it updates nicely so now we have one we could sprinkle in another widgets and bind that to our model and get a new model out and then we would display that one like that so this slider updates the first one we could also say okay maybe we want to but they just depend on the same widgets like that then we only have one and we could say maybe the second so it shows you the basics of creating small applications\", metadata={'source': 'BeBVdjENBZo'})]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = YoutubeLoader.from_youtube_url(yt_link[1])\n",
    "data_link = loader_yt.load()\n",
    "\n",
    "data_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_chunks = text_splitter.split_text(data_link[0].page_content)\n",
    "\n",
    "data = []\n",
    "for chunk in data_chunks:\n",
    "    data.append(Document(page_content=str(chunk)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"hi there thanks for taking the time in this video i'll give you a recap about a talk i recently gave at pi data copenhagen so i was invited to talk about an introduction to data apps with panel and since the event was not recorded i'll try to recap some of the things i did so first of all i gave an introduction to awesome panel which is the site i've created so panel is a framework for creating powerful data apps and for interactive data exploration in python and here at awesomepanel.org you can find inspiration you can find out if panel is something for you i believe it could be at least panel is something for me and i wrote a blog post about it you can check it out one of the amazing thing\",\n",
       " \"t it you can check it out one of the amazing things about panel is that it really works in the notebook and your editor so both in both places and this is like a very unique value proposition panel is open source for free and it's part of a broader family of systems that try to unite the pi data or pybis tools so at awesome panel you can also find a few galleries the app gallery is really all the applications that you see here in the sidebar let's check it out and the community gallery is more like the awesome list so this is the awesome panel gallery like this you can go in here find a lot of examples for inspiration you can find the code so a link to the code there might also be a video or\",\n",
       " \"a link to the code there might also be a video or other kinds of links in here for example we have up here so if you click this one the bootstrap dashboard for example you will go to a page where yeah you can just see the small example here and again we have the code here and you can just open it on github and take a look at how it's created yeah we had the awesome list or the community gallery as well so here you can get inspiration you can click and go to the resource and there might be additional links like github links for example this the github link here is the repository of this application and so on but what i really want to demonstrate in this video is the application that we were\",\n",
       " \"ate in this video is the application that we were building at pi there 22 so i just want to showcase how easy it is to build applications with panel so in order to use panel we need to import spn and now you can see i've started working here in vs code and that's because that's my preference i prefer working in this code but you can actually also work just as efficiently in a notebook if that's what you prefer so we can search here panel jugular panel preview and then we will see here that yeah we can get a nice efficient development environment as well in a notebook because we have this jupyter panel preview that reloads this application over here every time we save the notebook so you get\",\n",
       " \"r here every time we save the notebook so you get instant feedback and that's really important for your development experience but let's continue over here so the first thing you have to do every time is sort of import some of the things you need for panel especially in the notebook this is required and then we can make a basic hello world so hello hi that would be our message and here we really almost have a small application we can start up the server by writing panel serve the name of the script and then we can give it like an auto reload and show show will make it show up over here you can see it's blank now and that's because we need to mark the things we want to serve on our server as\",\n",
       " \"mark the things we want to serve on our server as serverable and if the screen is blank here the the code and server is not connected yet so we need to reload again now you can see we get it up here and the nice thing is that piano panel sort of recognizes stuff so this thing it now recognizes as a markdown string and it converts it nicely over here and again you see as soon as we save it's updated this is not the most beautiful application or anything so what we can do is we can add a template like this and now we can update the template so we can refer to the pn.template and then param.update so we want to update some parameters like the site is something for pi data copenhagen 2022 or jus\",\n",
       " \"te is something for pi data copenhagen 2022 or just pi data copenhagen i guess and the title is something like an introduction to data apps and you can see you get a nice thing here and what we could do furthermore is that we could actually add a sidebar so we could have some settings and make them servable so now we have like two cards here but i want to show them in the sidebar so i add area equal sidebar and then you get this now we could say the side width is too wide so we could set it to 200 pixels for example that's nice and we might also yeah that's fine okay so i want to show you data apps and data apps are normally interactive so they have widgets so the first thing we can do is we\",\n",
       " \"ey have widgets so the first thing we can do is we can say okay we probably have a some function or model so we can define a functional model and that could just return this one so now we get an error and that's because we we don't display anything yet so now we actually have our model and it returns the string and we see it over here now you can see it doesn't stretch nicely and that's because we need to say that we don't want fixed sizing on any panel we actually want a sizing mode called stretch width like that this model is not so interesting let's just give it a feature or give it a an argument so what about giving an argument five and saying now we want to do something like this so now\",\n",
       " \"aying now we want to do something like this so now you can see here it gets something like this one thing one problem here is that hmm count minus three and so on that's not so useful so we actually want to control the widget here so we can define the widget for the counts and panel has a lot of widgets including an end slider and here we could set the value equals five and star equals zero and equals five for example and make this servable so now we add an extra slider again we want to show it in the sidebar like this and we want to have the settings on top and now we need to connect or bind our model and now we did so we do that using pn.binds the model and then the count widget so we need\",\n",
       " \"nds the model and then the count widget so we need to find the model the argument called count that to the count we did and then we get a new interactive model activity out we can call that i model so now we have a custom widget over here let's give it a name as well like name equals count and you can see here now it's called count let's just check it out so we also have the dark theme that's nice the magic of the panel function is also it can like understand other kinds of objects like data frames so it can understand or interpret a lot of pi data's pi data's objects so import pandas spd so now we could have pd data frame instead and then say x equals [Music] it looks like we have something\",\n",
       " \"y x equals [Music] it looks like we have something here it should be a predict like this and now it's blank again then you need to refresh it like this so now you get a nice table and we can even say to the panel here that please please don't display the index nice and now you can see here and we can also here loading in decatur equals true so if we want a loading indicator we can get it so that every time we reload here we get the loading indicator and that's useful if you're sort of re-running the model model or the function is slow but let's just check this one out another thing you often want the parts one plotting library or framework you can use this hp plot so this h3plot.pandas when\",\n",
       " \"u can use this hp plot so this h3plot.pandas when we import that then it adds the dot hvplot interactive methods to pd data frame to the pandas data frame so now we can use that to say h3 plots let's see what happens so now we get plots and we can say color equals let's just find an accent let's make that accent ah it's because we put it inside the data frame we should we should put it inside like that so now we get the orange color here and we would actually like to have it for a template as well let's just run black to relay out this so that we can see what's going on like this [Music] so we can add a accent base color and a header background so now it looks like this and let's also make i\",\n",
       " \"nd so now it looks like this and let's also make it a little bit serious right so this is what we're looking at and yeah maybe we could even add logo okay we need some images and put it in a specific panel so that pinstrip t1 like this let's see how it looks so yeah now we didn't include it so we need to make it server and then i get some error down here so i need to like reload it a few times and i get something up here it's not really showing so let's say sizing mode let's see what happens it's not really showing anything it's it might be bad let's just copy this one okay let's see okay it downloaded it that was nice and i want to have it in the sidebar instead like that yes so now we have\",\n",
       " \"n the sidebar instead like that yes so now we have a nice application here you can see it updates when i drag the slider i showed you we could use pandas data frames we can use h3 plot we can also for example use plotting let's use partly express spx and then we need to change a few things here so first of all our data the data frame here then we need to say that block is px dot for example line and then we want the data here let's just see what happens so now it's downloading yes looks nice need something more like the ability to set the color and i don't color the screen sequence it probably is like this again it updates nicely so now we have one we could sprinkle in another widgets and bi\",\n",
       " 've one we could sprinkle in another widgets and bind that to our model and get a new model out and then we would display that one like that so this slider updates the first one we could also say okay maybe we want to but they just depend on the same widgets like that then we only have one and we could say maybe the second so it shows you the basics of creating small applications']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"hi there thanks for taking the time in this video i'll give you a recap about a talk i recently gave at pi data copenhagen so i was invited to talk about an introduction to data apps with panel and since the event was not recorded i'll try to recap some of the things i did so first of all i gave an introduction to awesome panel which is the site i've created so panel is a framework for creating powerful data apps and for interactive data exploration in python and here at awesomepanel.org you can find inspiration you can find out if panel is something for you i believe it could be at least panel is something for me and i wrote a blog post about it you can check it out one of the amazing thing\", metadata={}),\n",
       " Document(page_content=\"t it you can check it out one of the amazing things about panel is that it really works in the notebook and your editor so both in both places and this is like a very unique value proposition panel is open source for free and it's part of a broader family of systems that try to unite the pi data or pybis tools so at awesome panel you can also find a few galleries the app gallery is really all the applications that you see here in the sidebar let's check it out and the community gallery is more like the awesome list so this is the awesome panel gallery like this you can go in here find a lot of examples for inspiration you can find the code so a link to the code there might also be a video or\", metadata={}),\n",
       " Document(page_content=\"a link to the code there might also be a video or other kinds of links in here for example we have up here so if you click this one the bootstrap dashboard for example you will go to a page where yeah you can just see the small example here and again we have the code here and you can just open it on github and take a look at how it's created yeah we had the awesome list or the community gallery as well so here you can get inspiration you can click and go to the resource and there might be additional links like github links for example this the github link here is the repository of this application and so on but what i really want to demonstrate in this video is the application that we were\", metadata={}),\n",
       " Document(page_content=\"ate in this video is the application that we were building at pi there 22 so i just want to showcase how easy it is to build applications with panel so in order to use panel we need to import spn and now you can see i've started working here in vs code and that's because that's my preference i prefer working in this code but you can actually also work just as efficiently in a notebook if that's what you prefer so we can search here panel jugular panel preview and then we will see here that yeah we can get a nice efficient development environment as well in a notebook because we have this jupyter panel preview that reloads this application over here every time we save the notebook so you get\", metadata={}),\n",
       " Document(page_content=\"r here every time we save the notebook so you get instant feedback and that's really important for your development experience but let's continue over here so the first thing you have to do every time is sort of import some of the things you need for panel especially in the notebook this is required and then we can make a basic hello world so hello hi that would be our message and here we really almost have a small application we can start up the server by writing panel serve the name of the script and then we can give it like an auto reload and show show will make it show up over here you can see it's blank now and that's because we need to mark the things we want to serve on our server as\", metadata={}),\n",
       " Document(page_content=\"mark the things we want to serve on our server as serverable and if the screen is blank here the the code and server is not connected yet so we need to reload again now you can see we get it up here and the nice thing is that piano panel sort of recognizes stuff so this thing it now recognizes as a markdown string and it converts it nicely over here and again you see as soon as we save it's updated this is not the most beautiful application or anything so what we can do is we can add a template like this and now we can update the template so we can refer to the pn.template and then param.update so we want to update some parameters like the site is something for pi data copenhagen 2022 or jus\", metadata={}),\n",
       " Document(page_content=\"te is something for pi data copenhagen 2022 or just pi data copenhagen i guess and the title is something like an introduction to data apps and you can see you get a nice thing here and what we could do furthermore is that we could actually add a sidebar so we could have some settings and make them servable so now we have like two cards here but i want to show them in the sidebar so i add area equal sidebar and then you get this now we could say the side width is too wide so we could set it to 200 pixels for example that's nice and we might also yeah that's fine okay so i want to show you data apps and data apps are normally interactive so they have widgets so the first thing we can do is we\", metadata={}),\n",
       " Document(page_content=\"ey have widgets so the first thing we can do is we can say okay we probably have a some function or model so we can define a functional model and that could just return this one so now we get an error and that's because we we don't display anything yet so now we actually have our model and it returns the string and we see it over here now you can see it doesn't stretch nicely and that's because we need to say that we don't want fixed sizing on any panel we actually want a sizing mode called stretch width like that this model is not so interesting let's just give it a feature or give it a an argument so what about giving an argument five and saying now we want to do something like this so now\", metadata={}),\n",
       " Document(page_content=\"aying now we want to do something like this so now you can see here it gets something like this one thing one problem here is that hmm count minus three and so on that's not so useful so we actually want to control the widget here so we can define the widget for the counts and panel has a lot of widgets including an end slider and here we could set the value equals five and star equals zero and equals five for example and make this servable so now we add an extra slider again we want to show it in the sidebar like this and we want to have the settings on top and now we need to connect or bind our model and now we did so we do that using pn.binds the model and then the count widget so we need\", metadata={}),\n",
       " Document(page_content=\"nds the model and then the count widget so we need to find the model the argument called count that to the count we did and then we get a new interactive model activity out we can call that i model so now we have a custom widget over here let's give it a name as well like name equals count and you can see here now it's called count let's just check it out so we also have the dark theme that's nice the magic of the panel function is also it can like understand other kinds of objects like data frames so it can understand or interpret a lot of pi data's pi data's objects so import pandas spd so now we could have pd data frame instead and then say x equals [Music] it looks like we have something\", metadata={}),\n",
       " Document(page_content=\"y x equals [Music] it looks like we have something here it should be a predict like this and now it's blank again then you need to refresh it like this so now you get a nice table and we can even say to the panel here that please please don't display the index nice and now you can see here and we can also here loading in decatur equals true so if we want a loading indicator we can get it so that every time we reload here we get the loading indicator and that's useful if you're sort of re-running the model model or the function is slow but let's just check this one out another thing you often want the parts one plotting library or framework you can use this hp plot so this h3plot.pandas when\", metadata={}),\n",
       " Document(page_content=\"u can use this hp plot so this h3plot.pandas when we import that then it adds the dot hvplot interactive methods to pd data frame to the pandas data frame so now we can use that to say h3 plots let's see what happens so now we get plots and we can say color equals let's just find an accent let's make that accent ah it's because we put it inside the data frame we should we should put it inside like that so now we get the orange color here and we would actually like to have it for a template as well let's just run black to relay out this so that we can see what's going on like this [Music] so we can add a accent base color and a header background so now it looks like this and let's also make i\", metadata={}),\n",
       " Document(page_content=\"nd so now it looks like this and let's also make it a little bit serious right so this is what we're looking at and yeah maybe we could even add logo okay we need some images and put it in a specific panel so that pinstrip t1 like this let's see how it looks so yeah now we didn't include it so we need to make it server and then i get some error down here so i need to like reload it a few times and i get something up here it's not really showing so let's say sizing mode let's see what happens it's not really showing anything it's it might be bad let's just copy this one okay let's see okay it downloaded it that was nice and i want to have it in the sidebar instead like that yes so now we have\", metadata={}),\n",
       " Document(page_content=\"n the sidebar instead like that yes so now we have a nice application here you can see it updates when i drag the slider i showed you we could use pandas data frames we can use h3 plot we can also for example use plotting let's use partly express spx and then we need to change a few things here so first of all our data the data frame here then we need to say that block is px dot for example line and then we want the data here let's just see what happens so now it's downloading yes looks nice need something more like the ability to set the color and i don't color the screen sequence it probably is like this again it updates nicely so now we have one we could sprinkle in another widgets and bi\", metadata={}),\n",
       " Document(page_content='ve one we could sprinkle in another widgets and bind that to our model and get a new model out and then we would display that one like that so this slider updates the first one we could also say okay maybe we want to but they just depend on the same widgets like that then we only have one and we could say maybe the second so it shows you the basics of creating small applications', metadata={})]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Autor w filmie przedstawia framework Panel, który służy do tworzenia aplikacji danych i eksploracji interaktywnych danych w języku Python. Panel jest dostępny jako oprogramowanie open source i jest częścią rodziny narzędzi PyData. Autor prezentuje różne galerie i przykłady aplikacji, a następnie demonstruje, jak łatwo można zbudować aplikację z użyciem Panel. W filmie pokazuje, jak używać widgetów, jak połączyć je z modelem, jak wykorzystać bibliotekę Pandas i H3Plot do wizualizacji danych oraz jak dostosować wygląd aplikacji.'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "prompt_template = \"\"\"Write a concise summary of the following:\n",
    "\n",
    "\n",
    "{text}\n",
    "\n",
    "\n",
    "CONCISE SUMMARY IN POLISH:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
    "chain = load_summarize_chain(llm, chain_type=\"stuff\", prompt=PROMPT)\n",
    "chain.run(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'W prezentacji omówiono wykorzystanie analizy logów i algorytmów uczenia maszynowego do identyfikacji, analizy i przewidywania błędów w przemyśle produkcyjnym. Przedstawiono problem konserwacji frezarek dentystycznych oraz wykorzystanie danych logów i sensorów do przewidywania i zapobiegania błędom maszyn. Omówiono wykorzystanie techniki Word Embeddings oraz Gradient Boosting w procesie uczenia maszynowego. Przedstawiono korzyści wynikające z wykorzystania analizy logów, takie jak lepsze zrozumienie danych i możliwość identyfikacji błędów w czasie rzeczywistym, co pozwala na szybszą reakcję i poprawę jakości produktów.'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = YoutubeLoader.from_youtube_url(yt_link[5])\n",
    "data_link = loader.load()\n",
    "data_chunks = text_splitter.split_text(data_link[0].page_content)\n",
    "\n",
    "data = []\n",
    "for chunk in data_chunks:\n",
    "    data.append(Document(page_content=str(chunk)))\n",
    "\n",
    "prompt_template = \"\"\"Write a concise summary of the following video.\n",
    "Pay attention to what benefits we can take from a video.\n",
    "\n",
    "{text}\n",
    "\n",
    "\n",
    "CONCISE SUMMARY IN POLISH:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
    "chain = load_summarize_chain(llm, chain_type=\"stuff\", prompt=PROMPT)\n",
    "chain.run(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co dalej? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * możliwość zapisu pamięci czata do własnej analizy\n",
    "> * możliwość użycia agentów w celu wykonywania różnych zadań w zależności od kontekstu\n",
    "> * możliwość zapisu pamięci w postaci embeddingów do bazy wektorowej - wysyłanie odpowiedzi na podstawie już wcześniej przeprowadzonych rozmów, które przypadły nam do gustu i uznajemy za wartościowe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
